{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "import ale_py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn        \n",
    "import torch.optim as optim \n",
    "from torchsummary import summary\n",
    "\n",
    "import collections\n",
    "\n",
    "import wandb\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gymnasium version 1.0.0\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "ENV_NAME = \"ALE/Breakout-v5\"\n",
    "test_env = gym.make(ENV_NAME)\n",
    "\n",
    "print(test_env.unwrapped.get_action_meanings())\n",
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Source: M3-2_Example_1a (DQN on Pong, train)\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    #env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    \n",
    "    return env\n",
    "\n",
    "env=make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DQN(input_shape, output_shape):\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64*7*7, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, output_shape)\n",
    "    )\n",
    "    return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_REWARD_BOUND = 100\n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "EXPERIENCE_REPLAY_SIZE = 10000\n",
    "SYNC_TARGET_NETWORK = 1000\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999985\n",
    "EPS_MIN = 0.02\n",
    "\n",
    "INITIAL_BETA=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class PrioritizedExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, eps=0.001, alpha=0.6, beta=INITIAL_BETA):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "        # To make add priority to the experiences we add new attributes to the class\n",
    "        self.priorities = collections.deque(maxlen=capacity) # This indicates the priorities of the experiences\n",
    "        self.eps = eps  # This is a small constant to ensure no zero priority\n",
    "        self.alpha = alpha  # This is an exponent for scaling priorities\n",
    "        self.beta = beta  # This is and exponent for importance sampling adjustment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    # This function adds a new experience to the buffer with max priority\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        max_priority = max(self.priorities, default=1.0)\n",
    "        self.priorities.append(max_priority)\n",
    "\n",
    "    # This function calculates sampling probabilities for the buffer\n",
    "    def _get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities) ** self.alpha\n",
    "        return scaled_priorities / scaled_priorities.sum()\n",
    "\n",
    "    # This function calculates importance-sampling weights\n",
    "    def _get_importance(self, probabilities):\n",
    "        importance = ((1 / len(self.buffer)) * (1 / probabilities)) ** self.beta\n",
    "        importance_normalized = importance / importance.max()\n",
    "        return importance_normalized\n",
    "\n",
    "    # This function samples a batch of experiences from the buffer and returns the batch, importance weights, and indices for priority updates\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        sample_probs = self._get_probabilities()\n",
    "        sample_indices = np.random.choice(len(self.buffer), size=sample_size, p=sample_probs)\n",
    "\n",
    "        experiences = [self.buffer[idx] for idx in sample_indices]\n",
    "        importance = self._get_importance(sample_probs[sample_indices])\n",
    "\n",
    "        states, actions, rewards, dones, next_states = zip(*experiences)\n",
    "\n",
    "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.uint8), np.array(next_states)), importance, sample_indices\n",
    "\n",
    "    # This function updates priorities for the given indices using the errors and adds a small epsilon to ensure no priority is zero.\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priorities[idx] = (abs(error) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_ = np.array([self.current_state])\n",
    "            state = torch.tensor(state_).to(device)\n",
    "            q_vals = net(state)\n",
    "            _, act_ = torch.max(q_vals, dim=1)\n",
    "            action = int(act_.item())\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.current_state, action, reward, is_done, new_state)\n",
    "        self.exp_replay_buffer.append(exp)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\arroc\\OneDrive\\Escritorio\\Apuntes\\UAB\\3rd year\\P_of_ML\\Project\\Reinfrocement-Learning-Project\\src\\wandb\\run-20241130_133250-w7rk64os</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os' target=\"_blank\">celestial-wave-2</a></strong> to <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1cc43013450>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    project=\"Part1_DQN\",\n",
    "    config={\n",
    "        \"gamma\": GAMMA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"eps_start\": EPS_START,\n",
    "        \"eps_decay\": EPS_DECAY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training starts at  2024-11-30 13:32:58.729366\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:43 | Total games:1 | Mean reward: 0.000  (epsilon used: 1.00)\n",
      "Step:94 | Total games:2 | Mean reward: 0.500  (epsilon used: 1.00)\n",
      "Step:162 | Total games:3 | Mean reward: 1.333  (epsilon used: 1.00)\n",
      "Step:211 | Total games:4 | Mean reward: 1.250  (epsilon used: 1.00)\n",
      "Step:254 | Total games:5 | Mean reward: 1.000  (epsilon used: 1.00)\n",
      "Step:303 | Total games:6 | Mean reward: 1.000  (epsilon used: 1.00)\n",
      "Step:404 | Total games:7 | Mean reward: 1.571  (epsilon used: 0.99)\n",
      "Step:453 | Total games:8 | Mean reward: 1.500  (epsilon used: 0.99)\n",
      "Step:526 | Total games:9 | Mean reward: 1.667  (epsilon used: 0.99)\n",
      "Step:575 | Total games:10 | Mean reward: 1.600  (epsilon used: 0.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 43 that is less than the current step 265. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 94 that is less than the current step 265. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 162 that is less than the current step 265. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 211 that is less than the current step 265. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 254 that is less than the current step 265. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:653 | Total games:11 | Mean reward: 2.000  (epsilon used: 0.99)\n",
      "Step:698 | Total games:12 | Mean reward: 1.900  (epsilon used: 0.99)\n",
      "Step:761 | Total games:13 | Mean reward: 1.700  (epsilon used: 0.99)\n",
      "Step:815 | Total games:14 | Mean reward: 1.600  (epsilon used: 0.99)\n",
      "Step:892 | Total games:15 | Mean reward: 2.000  (epsilon used: 0.99)\n",
      "Step:960 | Total games:16 | Mean reward: 2.200  (epsilon used: 0.99)\n",
      "Step:1025 | Total games:17 | Mean reward: 2.000  (epsilon used: 0.98)\n",
      "Step:1119 | Total games:18 | Mean reward: 2.400  (epsilon used: 0.98)\n",
      "Step:1199 | Total games:19 | Mean reward: 2.500  (epsilon used: 0.98)\n",
      "Step:1272 | Total games:20 | Mean reward: 2.600  (epsilon used: 0.98)\n",
      "Step:1334 | Total games:21 | Mean reward: 2.300  (epsilon used: 0.98)\n",
      "Step:1394 | Total games:22 | Mean reward: 2.300  (epsilon used: 0.98)\n",
      "Step:1450 | Total games:23 | Mean reward: 2.400  (epsilon used: 0.98)\n",
      "Step:1511 | Total games:24 | Mean reward: 2.600  (epsilon used: 0.98)\n",
      "Step:1584 | Total games:25 | Mean reward: 2.300  (epsilon used: 0.98)\n",
      "Step:1630 | Total games:26 | Mean reward: 2.000  (epsilon used: 0.98)\n",
      "Step:1681 | Total games:27 | Mean reward: 1.800  (epsilon used: 0.98)\n",
      "Step:1757 | Total games:28 | Mean reward: 1.600  (epsilon used: 0.97)\n",
      "Step:1822 | Total games:29 | Mean reward: 1.400  (epsilon used: 0.97)\n",
      "Step:1880 | Total games:30 | Mean reward: 1.300  (epsilon used: 0.97)\n",
      "Step:1940 | Total games:31 | Mean reward: 1.300  (epsilon used: 0.97)\n",
      "Step:1995 | Total games:32 | Mean reward: 1.400  (epsilon used: 0.97)\n",
      "Step:2085 | Total games:33 | Mean reward: 1.600  (epsilon used: 0.97)\n",
      "Step:2134 | Total games:34 | Mean reward: 1.500  (epsilon used: 0.97)\n",
      "Step:2224 | Total games:35 | Mean reward: 1.900  (epsilon used: 0.97)\n",
      "Step:2296 | Total games:36 | Mean reward: 2.300  (epsilon used: 0.97)\n",
      "Step:2367 | Total games:37 | Mean reward: 2.500  (epsilon used: 0.97)\n",
      "Step:2420 | Total games:38 | Mean reward: 2.400  (epsilon used: 0.96)\n",
      "Step:2482 | Total games:39 | Mean reward: 2.400  (epsilon used: 0.96)\n",
      "Step:2556 | Total games:40 | Mean reward: 2.600  (epsilon used: 0.96)\n",
      "Step:2597 | Total games:41 | Mean reward: 2.500  (epsilon used: 0.96)\n",
      "Step:2655 | Total games:42 | Mean reward: 2.500  (epsilon used: 0.96)\n",
      "Step:2747 | Total games:43 | Mean reward: 2.500  (epsilon used: 0.96)\n",
      "Step:2823 | Total games:44 | Mean reward: 2.700  (epsilon used: 0.96)\n",
      "Step:2878 | Total games:45 | Mean reward: 2.300  (epsilon used: 0.96)\n",
      "Step:2914 | Total games:46 | Mean reward: 1.900  (epsilon used: 0.96)\n",
      "Step:2980 | Total games:47 | Mean reward: 1.800  (epsilon used: 0.96)\n",
      "Step:3021 | Total games:48 | Mean reward: 1.600  (epsilon used: 0.96)\n",
      "Step:3072 | Total games:49 | Mean reward: 1.500  (epsilon used: 0.95)\n",
      "Step:3133 | Total games:50 | Mean reward: 1.300  (epsilon used: 0.95)\n",
      "Step:3212 | Total games:51 | Mean reward: 1.600  (epsilon used: 0.95)\n",
      "Step:3264 | Total games:52 | Mean reward: 1.600  (epsilon used: 0.95)\n",
      "Step:3339 | Total games:53 | Mean reward: 1.400  (epsilon used: 0.95)\n",
      "Step:3399 | Total games:54 | Mean reward: 1.200  (epsilon used: 0.95)\n",
      "Step:3446 | Total games:55 | Mean reward: 1.100  (epsilon used: 0.95)\n",
      "Step:3507 | Total games:56 | Mean reward: 1.200  (epsilon used: 0.95)\n",
      "Step:3566 | Total games:57 | Mean reward: 1.200  (epsilon used: 0.95)\n",
      "Step:3646 | Total games:58 | Mean reward: 1.600  (epsilon used: 0.95)\n",
      "Step:3721 | Total games:59 | Mean reward: 2.000  (epsilon used: 0.95)\n",
      "Step:3800 | Total games:60 | Mean reward: 2.100  (epsilon used: 0.94)\n",
      "Step:3853 | Total games:61 | Mean reward: 1.800  (epsilon used: 0.94)\n",
      "Step:3918 | Total games:62 | Mean reward: 1.900  (epsilon used: 0.94)\n",
      "Step:3954 | Total games:63 | Mean reward: 1.700  (epsilon used: 0.94)\n",
      "Step:4034 | Total games:64 | Mean reward: 1.900  (epsilon used: 0.94)\n",
      "Step:4087 | Total games:65 | Mean reward: 1.900  (epsilon used: 0.94)\n",
      "Step:4127 | Total games:66 | Mean reward: 1.900  (epsilon used: 0.94)\n",
      "Step:4198 | Total games:67 | Mean reward: 1.900  (epsilon used: 0.94)\n",
      "Step:4247 | Total games:68 | Mean reward: 1.600  (epsilon used: 0.94)\n",
      "Step:4291 | Total games:69 | Mean reward: 1.100  (epsilon used: 0.94)\n",
      "Step:4365 | Total games:70 | Mean reward: 1.200  (epsilon used: 0.94)\n",
      "Step:4456 | Total games:71 | Mean reward: 1.700  (epsilon used: 0.94)\n",
      "Step:4503 | Total games:72 | Mean reward: 1.500  (epsilon used: 0.93)\n",
      "Step:4568 | Total games:73 | Mean reward: 1.700  (epsilon used: 0.93)\n",
      "Step:4648 | Total games:74 | Mean reward: 2.100  (epsilon used: 0.93)\n",
      "Step:4711 | Total games:75 | Mean reward: 2.200  (epsilon used: 0.93)\n",
      "Step:4765 | Total games:76 | Mean reward: 2.200  (epsilon used: 0.93)\n",
      "Step:4822 | Total games:77 | Mean reward: 2.200  (epsilon used: 0.93)\n",
      "Step:4893 | Total games:78 | Mean reward: 2.400  (epsilon used: 0.93)\n",
      "Step:4980 | Total games:79 | Mean reward: 2.700  (epsilon used: 0.93)\n",
      "Step:5047 | Total games:80 | Mean reward: 2.500  (epsilon used: 0.93)\n",
      "Step:5155 | Total games:81 | Mean reward: 2.700  (epsilon used: 0.93)\n",
      "Step:5217 | Total games:82 | Mean reward: 2.900  (epsilon used: 0.92)\n",
      "Step:5274 | Total games:83 | Mean reward: 2.900  (epsilon used: 0.92)\n",
      "Step:5370 | Total games:84 | Mean reward: 2.600  (epsilon used: 0.92)\n",
      "Step:5452 | Total games:85 | Mean reward: 2.800  (epsilon used: 0.92)\n",
      "Step:5501 | Total games:86 | Mean reward: 2.800  (epsilon used: 0.92)\n",
      "Step:5575 | Total games:87 | Mean reward: 2.700  (epsilon used: 0.92)\n",
      "Step:5656 | Total games:88 | Mean reward: 2.700  (epsilon used: 0.92)\n",
      "Step:5728 | Total games:89 | Mean reward: 2.700  (epsilon used: 0.92)\n",
      "Step:5795 | Total games:90 | Mean reward: 3.000  (epsilon used: 0.92)\n",
      "Step:5846 | Total games:91 | Mean reward: 2.400  (epsilon used: 0.92)\n",
      "Step:5937 | Total games:92 | Mean reward: 2.600  (epsilon used: 0.91)\n",
      "Step:6059 | Total games:93 | Mean reward: 3.000  (epsilon used: 0.91)\n",
      "Step:6119 | Total games:94 | Mean reward: 2.900  (epsilon used: 0.91)\n",
      "Step:6181 | Total games:95 | Mean reward: 2.800  (epsilon used: 0.91)\n",
      "Step:6260 | Total games:96 | Mean reward: 3.000  (epsilon used: 0.91)\n",
      "Step:6327 | Total games:97 | Mean reward: 2.900  (epsilon used: 0.91)\n",
      "Step:6399 | Total games:98 | Mean reward: 2.900  (epsilon used: 0.91)\n",
      "Step:6443 | Total games:99 | Mean reward: 2.600  (epsilon used: 0.91)\n",
      "Step:6507 | Total games:100 | Mean reward: 2.300  (epsilon used: 0.91)\n",
      "Step:6576 | Total games:101 | Mean reward: 2.500  (epsilon used: 0.91)\n",
      "Step:6685 | Total games:102 | Mean reward: 2.800  (epsilon used: 0.90)\n",
      "Step:6752 | Total games:103 | Mean reward: 2.400  (epsilon used: 0.90)\n",
      "Step:6824 | Total games:104 | Mean reward: 2.500  (epsilon used: 0.90)\n",
      "Step:6907 | Total games:105 | Mean reward: 2.600  (epsilon used: 0.90)\n",
      "Step:7006 | Total games:106 | Mean reward: 2.800  (epsilon used: 0.90)\n",
      "Step:7063 | Total games:107 | Mean reward: 2.800  (epsilon used: 0.90)\n",
      "Step:7114 | Total games:108 | Mean reward: 2.600  (epsilon used: 0.90)\n",
      "Step:7191 | Total games:109 | Mean reward: 3.000  (epsilon used: 0.90)\n",
      "Step:7245 | Total games:110 | Mean reward: 3.100  (epsilon used: 0.90)\n",
      "Step:7306 | Total games:111 | Mean reward: 3.000  (epsilon used: 0.90)\n",
      "Step:7384 | Total games:112 | Mean reward: 2.700  (epsilon used: 0.90)\n",
      "Step:7436 | Total games:113 | Mean reward: 2.500  (epsilon used: 0.89)\n",
      "Step:7499 | Total games:114 | Mean reward: 2.300  (epsilon used: 0.89)\n",
      "Step:7589 | Total games:115 | Mean reward: 2.600  (epsilon used: 0.89)\n",
      "Step:7650 | Total games:116 | Mean reward: 2.300  (epsilon used: 0.89)\n",
      "Step:7701 | Total games:117 | Mean reward: 2.400  (epsilon used: 0.89)\n",
      "Step:7789 | Total games:118 | Mean reward: 2.800  (epsilon used: 0.89)\n",
      "Step:7835 | Total games:119 | Mean reward: 2.500  (epsilon used: 0.89)\n",
      "Step:7928 | Total games:120 | Mean reward: 2.600  (epsilon used: 0.89)\n",
      "Step:7979 | Total games:121 | Mean reward: 2.500  (epsilon used: 0.89)\n",
      "Step:8050 | Total games:122 | Mean reward: 2.300  (epsilon used: 0.89)\n",
      "Step:8095 | Total games:123 | Mean reward: 2.400  (epsilon used: 0.89)\n",
      "Step:8164 | Total games:124 | Mean reward: 2.500  (epsilon used: 0.88)\n",
      "Step:8224 | Total games:125 | Mean reward: 2.000  (epsilon used: 0.88)\n",
      "Step:8328 | Total games:126 | Mean reward: 2.200  (epsilon used: 0.88)\n",
      "Step:8368 | Total games:127 | Mean reward: 2.100  (epsilon used: 0.88)\n",
      "Step:8414 | Total games:128 | Mean reward: 1.700  (epsilon used: 0.88)\n",
      "Step:8473 | Total games:129 | Mean reward: 1.800  (epsilon used: 0.88)\n",
      "Step:8558 | Total games:130 | Mean reward: 1.900  (epsilon used: 0.88)\n",
      "Step:8641 | Total games:131 | Mean reward: 2.100  (epsilon used: 0.88)\n",
      "Step:8742 | Total games:132 | Mean reward: 2.500  (epsilon used: 0.88)\n",
      "Step:8825 | Total games:133 | Mean reward: 2.600  (epsilon used: 0.88)\n",
      "Step:8892 | Total games:134 | Mean reward: 2.600  (epsilon used: 0.88)\n",
      "Step:8946 | Total games:135 | Mean reward: 2.600  (epsilon used: 0.87)\n",
      "Step:9000 | Total games:136 | Mean reward: 2.300  (epsilon used: 0.87)\n",
      "Step:9056 | Total games:137 | Mean reward: 2.500  (epsilon used: 0.87)\n",
      "Step:9126 | Total games:138 | Mean reward: 2.600  (epsilon used: 0.87)\n",
      "Step:9183 | Total games:139 | Mean reward: 2.500  (epsilon used: 0.87)\n",
      "Step:9256 | Total games:140 | Mean reward: 2.400  (epsilon used: 0.87)\n",
      "Step:9308 | Total games:141 | Mean reward: 2.200  (epsilon used: 0.87)\n",
      "Step:9411 | Total games:142 | Mean reward: 2.100  (epsilon used: 0.87)\n",
      "Step:9456 | Total games:143 | Mean reward: 2.000  (epsilon used: 0.87)\n",
      "Step:9541 | Total games:144 | Mean reward: 2.100  (epsilon used: 0.87)\n",
      "Step:9612 | Total games:145 | Mean reward: 2.300  (epsilon used: 0.87)\n",
      "Step:9673 | Total games:146 | Mean reward: 2.400  (epsilon used: 0.86)\n",
      "Step:9780 | Total games:147 | Mean reward: 2.800  (epsilon used: 0.86)\n",
      "Step:9834 | Total games:148 | Mean reward: 2.700  (epsilon used: 0.86)\n",
      "Step:9900 | Total games:149 | Mean reward: 2.900  (epsilon used: 0.86)\n",
      "Step:9966 | Total games:150 | Mean reward: 2.700  (epsilon used: 0.86)\n",
      "Step:10028 | Total games:151 | Mean reward: 2.700  (epsilon used: 0.86)\n",
      "Step:10077 | Total games:152 | Mean reward: 2.300  (epsilon used: 0.86)\n",
      "Step:10130 | Total games:153 | Mean reward: 2.300  (epsilon used: 0.86)\n",
      "Step:10181 | Total games:154 | Mean reward: 2.000  (epsilon used: 0.86)\n",
      "Step:10253 | Total games:155 | Mean reward: 1.900  (epsilon used: 0.86)\n",
      "Step:10318 | Total games:156 | Mean reward: 2.000  (epsilon used: 0.86)\n",
      "Step:10373 | Total games:157 | Mean reward: 1.500  (epsilon used: 0.86)\n",
      "Step:10463 | Total games:158 | Mean reward: 1.700  (epsilon used: 0.85)\n",
      "Step:10518 | Total games:159 | Mean reward: 1.500  (epsilon used: 0.85)\n",
      "Step:10605 | Total games:160 | Mean reward: 1.800  (epsilon used: 0.85)\n",
      "Step:10662 | Total games:161 | Mean reward: 1.800  (epsilon used: 0.85)\n",
      "Step:10708 | Total games:162 | Mean reward: 1.700  (epsilon used: 0.85)\n",
      "Step:10773 | Total games:163 | Mean reward: 1.800  (epsilon used: 0.85)\n",
      "Step:10815 | Total games:164 | Mean reward: 1.700  (epsilon used: 0.85)\n",
      "Step:10872 | Total games:165 | Mean reward: 1.700  (epsilon used: 0.85)\n",
      "Step:10929 | Total games:166 | Mean reward: 1.600  (epsilon used: 0.85)\n",
      "Step:10978 | Total games:167 | Mean reward: 1.600  (epsilon used: 0.85)\n",
      "Step:11047 | Total games:168 | Mean reward: 1.600  (epsilon used: 0.85)\n",
      "Step:11090 | Total games:169 | Mean reward: 1.500  (epsilon used: 0.85)\n",
      "Step:11168 | Total games:170 | Mean reward: 1.400  (epsilon used: 0.85)\n",
      "Step:11225 | Total games:171 | Mean reward: 1.500  (epsilon used: 0.85)\n",
      "Step:11333 | Total games:172 | Mean reward: 1.800  (epsilon used: 0.84)\n",
      "Step:11389 | Total games:173 | Mean reward: 1.600  (epsilon used: 0.84)\n",
      "Step:11449 | Total games:174 | Mean reward: 1.700  (epsilon used: 0.84)\n",
      "Step:11527 | Total games:175 | Mean reward: 1.800  (epsilon used: 0.84)\n",
      "Step:11593 | Total games:176 | Mean reward: 1.800  (epsilon used: 0.84)\n",
      "Step:11660 | Total games:177 | Mean reward: 2.000  (epsilon used: 0.84)\n",
      "Step:11725 | Total games:178 | Mean reward: 1.900  (epsilon used: 0.84)\n",
      "Step:11786 | Total games:179 | Mean reward: 1.900  (epsilon used: 0.84)\n",
      "Step:11846 | Total games:180 | Mean reward: 1.800  (epsilon used: 0.84)\n",
      "Step:11904 | Total games:181 | Mean reward: 1.900  (epsilon used: 0.84)\n",
      "Step:11970 | Total games:182 | Mean reward: 1.800  (epsilon used: 0.84)\n",
      "Step:12014 | Total games:183 | Mean reward: 1.900  (epsilon used: 0.84)\n",
      "Step:12074 | Total games:184 | Mean reward: 2.100  (epsilon used: 0.83)\n",
      "Step:12115 | Total games:185 | Mean reward: 1.800  (epsilon used: 0.83)\n",
      "Step:12203 | Total games:186 | Mean reward: 1.900  (epsilon used: 0.83)\n",
      "Step:12289 | Total games:187 | Mean reward: 1.900  (epsilon used: 0.83)\n",
      "Step:12354 | Total games:188 | Mean reward: 1.900  (epsilon used: 0.83)\n",
      "Step:12458 | Total games:189 | Mean reward: 2.100  (epsilon used: 0.83)\n",
      "Step:12507 | Total games:190 | Mean reward: 1.900  (epsilon used: 0.83)\n",
      "Step:12589 | Total games:191 | Mean reward: 1.900  (epsilon used: 0.83)\n",
      "Step:12670 | Total games:192 | Mean reward: 2.000  (epsilon used: 0.83)\n",
      "Step:12720 | Total games:193 | Mean reward: 2.000  (epsilon used: 0.83)\n",
      "Step:12780 | Total games:194 | Mean reward: 1.700  (epsilon used: 0.83)\n",
      "Step:12819 | Total games:195 | Mean reward: 1.700  (epsilon used: 0.83)\n",
      "Step:12895 | Total games:196 | Mean reward: 1.700  (epsilon used: 0.82)\n",
      "Step:12973 | Total games:197 | Mean reward: 1.600  (epsilon used: 0.82)\n",
      "Step:13028 | Total games:198 | Mean reward: 1.500  (epsilon used: 0.82)\n",
      "Step:13092 | Total games:199 | Mean reward: 1.500  (epsilon used: 0.82)\n",
      "Step:13144 | Total games:200 | Mean reward: 1.600  (epsilon used: 0.82)\n",
      "Step:13195 | Total games:201 | Mean reward: 1.300  (epsilon used: 0.82)\n",
      "Step:13274 | Total games:202 | Mean reward: 1.200  (epsilon used: 0.82)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 68\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_number \u001b[38;5;241m%\u001b[39m SYNC_TARGET_NETWORK \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     71\u001b[0m     target_net\u001b[38;5;241m.\u001b[39mload_state_dict(net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     adam(\n\u001b[0;32m    224\u001b[0m         params_with_grad,\n\u001b[0;32m    225\u001b[0m         grads,\n\u001b[0;32m    226\u001b[0m         exp_avgs,\n\u001b[0;32m    227\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    228\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    229\u001b[0m         state_steps,\n\u001b[0;32m    230\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    231\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    232\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    233\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    234\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    235\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    236\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    237\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    238\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    239\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    240\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    241\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    242\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    243\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    244\u001b[0m     )\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m func(\n\u001b[0;32m    785\u001b[0m     params,\n\u001b[0;32m    786\u001b[0m     grads,\n\u001b[0;32m    787\u001b[0m     exp_avgs,\n\u001b[0;32m    788\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    789\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    790\u001b[0m     state_steps,\n\u001b[0;32m    791\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    792\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    793\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    794\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    795\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    796\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    797\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    798\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    799\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    800\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    801\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    802\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    803\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:369\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    367\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m--> 369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[0;32m    370\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    371\u001b[0m     exp_avg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = make_DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "# Here we replace standard buffer with the PER buffer\n",
    "priorized_buffer = PrioritizedExperienceReplayBuffer(EXPERIENCE_REPLAY_SIZE)\n",
    "agent = DQNAgent(env, priorized_buffer)\n",
    "\n",
    "epsilon = EPS_START\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "losses = []\n",
    "step_number = 0\n",
    "\n",
    "while True:\n",
    "    step_number += 1\n",
    "    epsilon = max(epsilon * EPS_DECAY, EPS_MIN)\n",
    "\n",
    "    # Here we update beta from INITIAL_BETA to 1.0\n",
    "    priorized_buffer.beta = min(1.0, priorized_buffer.beta + (1.0 - INITIAL_BETA) / 120000) # This is divided by 120000 because this is the expected number of steps for the algorithm to run. In this way, beta will be 1.0 when the algorithm reach the 120000 steps\n",
    "\n",
    "\n",
    "    reward = agent.step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "\n",
    "        print(f\"Step:{step_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f}  (epsilon used: {epsilon:.2f})\")\n",
    "        wandb.log({\"epsilon\": epsilon, \"reward_100\": mean_reward, \"reward\": reward}, step=step_number)\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(f\"SOLVED in {step_number} steps and {len(total_rewards)} games\")\n",
    "            break\n",
    "\n",
    "    if len(priorized_buffer) < EXPERIENCE_REPLAY_SIZE:\n",
    "        continue\n",
    "\n",
    "    (states_, actions_, rewards_, dones_, next_states_), importance, indices = priorized_buffer.sample(BATCH_SIZE)\n",
    "    importance = torch.tensor(importance, dtype=torch.float32).to(device)\n",
    "\n",
    "    states = torch.tensor(states_).to(device)\n",
    "    next_states = torch.tensor(next_states_).to(device)\n",
    "    actions = torch.tensor(actions_).to(device)\n",
    "    rewards = torch.tensor(rewards_).to(device)\n",
    "    dones = torch.BoolTensor(dones_).to(device)\n",
    "\n",
    "    Q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    next_state_values = target_net(next_states).max(1)[0]\n",
    "    next_state_values[dones] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_Q_values = next_state_values * GAMMA + rewards\n",
    "\n",
    "\n",
    "    errors = torch.abs(Q_values - expected_Q_values).detach().cpu().numpy()  # Here we calculate the TD errors\n",
    "    loss = (importance * nn.MSELoss(reduction='none')(Q_values, expected_Q_values)).mean() # Here we use reduction='none' because we want to calculate the MSE element by element first, multiply it by the importance weights, and then take the mean\n",
    "\n",
    "    # We update the buffer priorities based on the TD errors\n",
    "    priorized_buffer.update_priorities(indices, errors)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    mean_losses = np.mean(losses[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "    wandb.log({\"loss_100\": mean_losses, \"loss\": loss.item()}, step=step_number)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step_number % SYNC_TARGET_NETWORK == 0:\n",
    "        target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"../models/Part1_DQN.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training ends at  2024-11-30 13:38:24.016023\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cc62a4fe274abe9749fa9c1cf21e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.026 MB of 0.026 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>████▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>loss</td><td>█▃▇▇██▃▃▂▃▇▃▂▅▅▄▂▂▂▂▂▄▂▂▃▂▂▁▂▂▁▂▂▁▂▃▂▅▂▂</td></tr><tr><td>loss_100</td><td>▃▃▆▄▆█▇▄▅▃▃▂▂▁▂▂▂▃▅▃▃▃▂▃▂▂▁▂▂▂▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>reward</td><td>▃▄▄▅▃▂▄▃▅▁▂▁▂▃▂▁█▄▃▄▇▄▁█▃▅▆▃▂▃▂▂▄▂▃▃▂▁▄▂</td></tr><tr><td>reward_100</td><td>▂▃▂▃▃▆▆▇▄▂▅▃▂▁▃▁▅▇▇█▆▇█▇▆▄▇▅▆▅▆▇█▄▃▃▄▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epsilon</td><td>0.81946</td></tr><tr><td>loss</td><td>0.01461</td></tr><tr><td>loss_100</td><td>0.0098</td></tr><tr><td>reward</td><td>2</td></tr><tr><td>reward_100</td><td>1.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-wave-2</strong> at: <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/w7rk64os</a><br/> View project at: <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241130_133250-w7rk64os\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
