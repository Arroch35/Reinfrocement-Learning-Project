{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:20.862256Z",
     "iopub.status.busy": "2024-12-05T19:34:20.861862Z",
     "iopub.status.idle": "2024-12-05T19:34:55.658603Z",
     "shell.execute_reply": "2024-12-05T19:34:55.657315Z",
     "shell.execute_reply.started": "2024-12-05T19:34:20.862181Z"
    },
    "id": "mWC6p2mus1Pp",
    "outputId": "3a6aa254-b719-4c3a-a734-220ec36cc272",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium==1.0.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gymnasium==1.0.0) (0.0.4)\n",
      "Requirement already satisfied: ale-py in c:\\users\\arroc\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>1.20 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from ale-py) (1.24.3)\n",
      "Requirement already satisfied: wandb in c:\\users\\arroc\\anaconda3\\lib\\site-packages (0.18.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (2.5.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\arroc\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\arroc\\anaconda3\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "# imports:\n",
    "!pip install gymnasium==1.0.0\n",
    "!pip install ale-py\n",
    "!pip install wandb\n",
    "!pip install torchsummary\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import collections\n",
    "\n",
    "import wandb\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:55.667886Z",
     "iopub.status.busy": "2024-12-05T19:34:55.667566Z",
     "iopub.status.idle": "2024-12-05T19:34:55.866626Z",
     "shell.execute_reply": "2024-12-05T19:34:55.865613Z",
     "shell.execute_reply.started": "2024-12-05T19:34:55.667841Z"
    },
    "id": "4Dh5uEFps1Pr",
    "outputId": "e0de4afb-9744-4b6e-ec2a-cd32ff23d811",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Gymnasium version 1.0.0\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "ENV_NAME = \"ALE/Breakout-v5\"\n",
    "test_env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "\n",
    "print(test_env.unwrapped.get_action_meanings())\n",
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:55.869533Z",
     "iopub.status.busy": "2024-12-05T19:34:55.869167Z",
     "iopub.status.idle": "2024-12-05T19:34:56.073889Z",
     "shell.execute_reply": "2024-12-05T19:34:56.072750Z",
     "shell.execute_reply.started": "2024-12-05T19:34:55.869501Z"
    },
    "id": "NrXvDPuTs1Pt",
    "outputId": "cd43d67c-9b1a-4414-9651-a3efe1abd4f2",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160, 3)\n",
      "MaxAndSkipObservation: (210, 160, 3)\n",
      "ResizeObservation    : (84, 84, 3)\n",
      "GrayscaleObservation : (84, 84, 1)\n",
      "ImageToPyTorch       : (1, 84, 84)\n",
      "ReshapeObservation   : (84, 84)\n",
      "FrameStackObservation: (4, 84, 84)\n",
      "ScaledFloatFrame     : (4, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "# Source: This class domes from the class activity M3-2_Example_1a (DQN on Pong, train)\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "# Source: This class domes from the class activity M3-2_Example_1a (DQN on Pong, train)\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "# Source: This class has been adapted from the following github repository: # https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
    "        if terminated or truncated:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        obs, _, terminated, truncated, _ = self.env.step(2)\n",
    "        if terminated or truncated:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        return obs, info\n",
    "\n",
    "# Source: This class has been adapted from the following github repository: # https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done  = True\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.was_real_done = terminated or truncated\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            terminated = True\n",
    "            truncated = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _, info = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs, info\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "    #env = EpisodicLifeEnv(env)\n",
    "\n",
    "    return env\n",
    "\n",
    "env=make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.075946Z",
     "iopub.status.busy": "2024-12-05T19:34:56.075609Z",
     "iopub.status.idle": "2024-12-05T19:34:56.083507Z",
     "shell.execute_reply": "2024-12-05T19:34:56.082692Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.075916Z"
    },
    "id": "38kSfg7Ds1Pt",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.086055Z",
     "iopub.status.busy": "2024-12-05T19:34:56.085298Z",
     "iopub.status.idle": "2024-12-05T19:34:56.107735Z",
     "shell.execute_reply": "2024-12-05T19:34:56.106825Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.086007Z"
    },
    "id": "ezW1l_FOAcoH",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Source: This class comes from the following github repository: https://github.com/SimonNick/rainbow/blob/master/model.py\n",
    "class NoisyLinear(nn.Module):\n",
    "    # This class is a linear layer with added noise\n",
    "    def __init__(self, in_features, out_features, sigma_init):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features \n",
    "        self.sigma_init = sigma_init\n",
    "\n",
    "        # These are the mean parameters for weights and biases\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "\n",
    "        # This is a temporary buffer for sampling noise\n",
    "        self.register_buffer('sample_weight_in', torch.FloatTensor(in_features))\n",
    "        self.register_buffer('sample_weight_out', torch.FloatTensor(out_features))\n",
    "        self.register_buffer('sample_bias_out', torch.FloatTensor(out_features))\n",
    "\n",
    "        # Her we initialize the parameters ang generate initial noise\n",
    "        self.reset_parameters()\n",
    "        self.sample_noise()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # During training, add noise to weights and biases\n",
    "            weight = self.weight_mu + self.weight_sigma.mul(self.weight_epsilon)\n",
    "            bias = self.bias_mu + self.bias_sigma.mul(self.bias_epsilon)\n",
    "        else:\n",
    "            # During evaluation, use the deterministic weights and biases\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        # We perform the linear transformation\n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "\n",
    "        # Range for uniform initialization of the mean parameters\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "\n",
    "        # Here we initialize weights and biases with a uniform distribution\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.sigma_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.sigma_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def sample_noise(self):\n",
    "\n",
    "        # We generate noise for the input and output dimensions of the weights\n",
    "        self.sample_weight_in = self._scale_noise(self.sample_weight_in)\n",
    "        self.sample_weight_out = self._scale_noise(self.sample_weight_out)\n",
    "        self.sample_bias_out = self._scale_noise(self.sample_bias_out)\n",
    "\n",
    "        # We compute element-wise noise for weights and biases\n",
    "        self.weight_epsilon.copy_(self.sample_weight_out.ger(self.sample_weight_in))\n",
    "        self.bias_epsilon.copy_(self.sample_bias_out)\n",
    "    \n",
    "    def _scale_noise(self, x):\n",
    "\n",
    "        # We generate Gaussian noise\n",
    "        x = x.normal_()\n",
    "        \n",
    "        # Here we apply scaling: sign(x) * sqrt(abs(x)). This scaling ensures noise has zero mean and unit variance\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x\n",
    "\n",
    "# Source: This code is adapted form the following github repository: https://github.com/dxyang/DQN_pytorch/blob/master/model.py\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # These are the common feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # These are the advantage stream layers\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "        # These are the value stream\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First, we extract the features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Then, we compute advantage and value streams\n",
    "        adv = self.advantage(features)\n",
    "        val = self.value(features).expand(x.size(0), self.num_actions)\n",
    "\n",
    "        # Finally, we combine streams into Q-values\n",
    "        q_values = val + adv - adv.mean(dim=1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "class NoisyDuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, sigma_init=0.5):\n",
    "        super(NoisyDuelingDQN, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # These are the common feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # These are the advantage stream layers with noisy layers\n",
    "        self.advantage = nn.Sequential(\n",
    "            NoisyLinear(64 * 7 * 7, 512, sigma_init),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, num_actions, sigma_init)\n",
    "        )\n",
    "\n",
    "        # These are the value stream with noisy layers\n",
    "        self.value = nn.Sequential(\n",
    "            NoisyLinear(64 * 7 * 7, 512, sigma_init),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, 1, sigma_init)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First, we extract the features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # Then, we compute advantage and value streams\n",
    "        adv = self.advantage(features)\n",
    "        val = self.value(features).expand(x.size(0), self.num_actions)\n",
    "\n",
    "        # Finally, we combine streams into Q-values\n",
    "        q_values = val + adv - adv.mean(dim=1, keepdim=True)\n",
    "        return q_values\n",
    "\n",
    "    def reset_noise(self):\n",
    "        # Here we reset noise for all noisy layers\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, NoisyLinear):\n",
    "                layer.sample_noise()\n",
    "            elif isinstance(layer, nn.Sequential):\n",
    "                for sub_layer in layer:\n",
    "                    if isinstance(sub_layer, NoisyLinear):\n",
    "                        sub_layer.sample_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.109856Z",
     "iopub.status.busy": "2024-12-05T19:34:56.109104Z",
     "iopub.status.idle": "2024-12-05T19:34:56.122486Z",
     "shell.execute_reply": "2024-12-05T19:34:56.121388Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.109807Z"
    },
    "id": "Hdf6SWo-s1Pu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MEAN_REWARD_BOUND = 700         # Max is 864\n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "EXPERIENCE_REPLAY_SIZE = 50000\n",
    "SYNC_TARGET_NETWORK = 1000\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_DECAY = 0.999985\n",
    "EPS_MIN = 0.05\n",
    "\n",
    "INITIAL_BETA=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.124439Z",
     "iopub.status.busy": "2024-12-05T19:34:56.123911Z",
     "iopub.status.idle": "2024-12-05T19:34:56.134917Z",
     "shell.execute_reply": "2024-12-05T19:34:56.133896Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.124393Z"
    },
    "id": "dOM4YSp9s1Pv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Source: This code comes from the class activity M3-2_Example_1a (DQN on Pong, train)\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, BATCH_SIZE):\n",
    "        indices = np.random.choice(len(self.buffer), BATCH_SIZE, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "    \n",
    "\n",
    "# Source: Code adapted from the following github repository: https://github.com/the-computer-scientist/OpenAIGym/blob/master/PrioritizedExperienceReplayInOpenAIGym.ipynb\n",
    "class PrioritizedExperienceReplayBuffer:\n",
    "    def __init__(self, capacity, eps=0.001, alpha=0.6, beta=INITIAL_BETA):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "        # To make add priority to the experiences we add new attributes to the class\n",
    "        self.priorities = collections.deque(maxlen=capacity) # This indicates the priorities of the experiences\n",
    "        self.eps = eps  # This is a small constant to ensure no zero priority\n",
    "        self.alpha = alpha  # This is an exponent for scaling priorities\n",
    "        self.beta = beta  # This is and exponent for importance sampling adjustment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    # This function adds a new experience to the buffer with max priority\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        max_priority = max(self.priorities, default=1.0)\n",
    "        self.priorities.append(max_priority)\n",
    "\n",
    "    # This function calculates sampling probabilities for the buffer\n",
    "    def _get_probabilities(self):\n",
    "        scaled_priorities = np.array(self.priorities) ** self.alpha\n",
    "        return scaled_priorities / scaled_priorities.sum()\n",
    "\n",
    "    # This function calculates importance-sampling weights\n",
    "    def _get_importance(self, probabilities):\n",
    "        importance = ((1 / len(self.buffer)) * (1 / probabilities)) ** self.beta\n",
    "        importance_normalized = importance / importance.max()\n",
    "        return importance_normalized\n",
    "\n",
    "    # This function samples a batch of experiences from the buffer and returns the batch, importance weights, and indices for priority updates\n",
    "    def sample(self, batch_size):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        sample_probs = self._get_probabilities()\n",
    "        sample_indices = np.random.choice(len(self.buffer), size=sample_size, p=sample_probs)\n",
    "\n",
    "        experiences = [self.buffer[idx] for idx in sample_indices]\n",
    "        importance = self._get_importance(sample_probs[sample_indices])\n",
    "\n",
    "        states, actions, rewards, dones, next_states = zip(*experiences)\n",
    "\n",
    "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.uint8), np.array(next_states)), importance, sample_indices\n",
    "\n",
    "    # This function updates priorities for the given indices using the errors and adds a small epsilon to ensure no priority is zero.\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            self.priorities[idx] = (abs(error) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.136794Z",
     "iopub.status.busy": "2024-12-05T19:34:56.136444Z",
     "iopub.status.idle": "2024-12-05T19:34:56.147406Z",
     "shell.execute_reply": "2024-12-05T19:34:56.146487Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.136747Z"
    },
    "id": "oTVzkdvQs1Pv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Source: This code comes from the class activity M3-2_Example_1a (DQN on Pong, train)\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, exp_replay_buffer):\n",
    "        self.env = env\n",
    "        self.exp_replay_buffer = exp_replay_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.current_state = self.env.reset()[0]\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, net, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        state_ = np.array([self.current_state])\n",
    "        state = torch.tensor(state_).to(device)\n",
    "        q_vals = net(state)\n",
    "        _, act_ = torch.max(q_vals, dim=1)\n",
    "        action = int(act_.item())\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "\n",
    "        clipped_reward=np.sign(reward) # Here we apply reward clipping\n",
    "\n",
    "        exp = Experience(self.current_state, action, clipped_reward, is_done, new_state)\n",
    "        self.exp_replay_buffer.append(exp)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:56.150214Z",
     "iopub.status.busy": "2024-12-05T19:34:56.149859Z",
     "iopub.status.idle": "2024-12-05T19:34:57.334175Z",
     "shell.execute_reply": "2024-12-05T19:34:57.333238Z",
     "shell.execute_reply.started": "2024-12-05T19:34:56.150171Z"
    },
    "id": "6IWKuCozs1Pw",
    "outputId": "34029e77-2795-4634-c3d1-5cd0e576391c",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marroch35\u001b[0m (\u001b[33marroch35-organitzation\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\arroc\\OneDrive\\Escritorio\\Apuntes\\UAB\\3rd year\\P_of_ML\\Project\\Reinfrocement-Learning-Project\\src\\wandb\\run-20241206_140002-8jwwxw0a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a' target=\"_blank\">eager-breeze-43</a></strong> to <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a6af88ecd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    project=\"Part1_DQN\",\n",
    "    config={\n",
    "        \"gamma\": GAMMA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"eps_start\": EPS_START,\n",
    "        \"eps_decay\": EPS_DECAY,\n",
    "        \"expereince_replay_size\": EXPERIENCE_REPLAY_SIZE,\n",
    "        \"sync_target_network\": SYNC_TARGET_NETWORK\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:34:57.335960Z",
     "iopub.status.busy": "2024-12-05T19:34:57.335530Z",
     "iopub.status.idle": "2024-12-05T19:34:57.342547Z",
     "shell.execute_reply": "2024-12-05T19:34:57.341579Z",
     "shell.execute_reply.started": "2024-12-05T19:34:57.335906Z"
    },
    "id": "-D-cp1Ems1Px",
    "outputId": "4040c92c-603f-46a7-9965-12269a1cb445",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training starts at  2024-12-06 14:00:04.318956\n"
     ]
    }
   ],
   "source": [
    "print(\">>> Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T19:35:27.026249Z",
     "iopub.status.busy": "2024-12-05T19:35:27.025431Z"
    },
    "id": "axLmUZI9s1Px",
    "outputId": "9f0551ff-9906-4476-bbbc-f5644b1818d0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:70 | Total games:1 | Mean reward: 4.000  \n",
      "Step:130 | Total games:2 | Mean reward: 3.000  \n",
      "Step:188 | Total games:3 | Mean reward: 2.667  \n",
      "Step:264 | Total games:4 | Mean reward: 2.750  \n",
      "Step:308 | Total games:5 | Mean reward: 2.400  \n",
      "Step:352 | Total games:6 | Mean reward: 2.167  \n",
      "Step:401 | Total games:7 | Mean reward: 2.143  \n",
      "Step:465 | Total games:8 | Mean reward: 2.250  \n",
      "Step:509 | Total games:9 | Mean reward: 2.111  \n",
      "Step:591 | Total games:10 | Mean reward: 2.300  \n",
      "Step:667 | Total games:11 | Mean reward: 2.300  \n",
      "Step:716 | Total games:12 | Mean reward: 2.200  \n",
      "Step:747 | Total games:13 | Mean reward: 2.000  \n",
      "Step:790 | Total games:14 | Mean reward: 1.800  \n",
      "Step:860 | Total games:15 | Mean reward: 2.100  \n",
      "Step:904 | Total games:16 | Mean reward: 2.100  \n",
      "Step:948 | Total games:17 | Mean reward: 2.000  \n",
      "Step:1013 | Total games:18 | Mean reward: 2.000  \n",
      "Step:1098 | Total games:19 | Mean reward: 2.200  \n",
      "Step:1150 | Total games:20 | Mean reward: 2.000  \n",
      "Step:1206 | Total games:21 | Mean reward: 1.800  \n",
      "Step:1260 | Total games:22 | Mean reward: 1.900  \n",
      "Step:1316 | Total games:23 | Mean reward: 2.200  \n",
      "Step:1377 | Total games:24 | Mean reward: 2.400  \n",
      "Step:1430 | Total games:25 | Mean reward: 2.100  \n",
      "Step:1478 | Total games:26 | Mean reward: 2.200  \n",
      "Step:1541 | Total games:27 | Mean reward: 2.400  \n",
      "Step:1579 | Total games:28 | Mean reward: 2.100  \n",
      "Step:1613 | Total games:29 | Mean reward: 1.800  \n",
      "Step:1658 | Total games:30 | Mean reward: 1.700  \n",
      "Step:1719 | Total games:31 | Mean reward: 1.800  \n",
      "Step:1772 | Total games:32 | Mean reward: 1.800  \n",
      "Step:1818 | Total games:33 | Mean reward: 1.600  \n",
      "Step:1853 | Total games:34 | Mean reward: 1.300  \n",
      "Step:1932 | Total games:35 | Mean reward: 1.600  \n",
      "Step:1987 | Total games:36 | Mean reward: 1.600  \n",
      "Step:2026 | Total games:37 | Mean reward: 1.300  \n",
      "Step:2074 | Total games:38 | Mean reward: 1.500  \n",
      "Step:2105 | Total games:39 | Mean reward: 1.500  \n",
      "Step:2137 | Total games:40 | Mean reward: 1.400  \n",
      "Step:2205 | Total games:41 | Mean reward: 1.400  \n",
      "Step:2255 | Total games:42 | Mean reward: 1.400  \n",
      "Step:2305 | Total games:43 | Mean reward: 1.400  \n",
      "Step:2358 | Total games:44 | Mean reward: 1.600  \n",
      "Step:2410 | Total games:45 | Mean reward: 1.300  \n",
      "Step:2457 | Total games:46 | Mean reward: 1.200  \n",
      "Step:2505 | Total games:47 | Mean reward: 1.400  \n",
      "Step:2537 | Total games:48 | Mean reward: 1.200  \n",
      "Step:2595 | Total games:49 | Mean reward: 1.400  \n",
      "Step:2624 | Total games:50 | Mean reward: 1.400  \n",
      "Step:2692 | Total games:51 | Mean reward: 1.500  \n",
      "Step:2745 | Total games:52 | Mean reward: 1.500  \n",
      "Step:2796 | Total games:53 | Mean reward: 1.600  \n",
      "Step:2830 | Total games:54 | Mean reward: 1.400  \n",
      "Step:2879 | Total games:55 | Mean reward: 1.500  \n",
      "Step:2925 | Total games:56 | Mean reward: 1.500  \n",
      "Step:2984 | Total games:57 | Mean reward: 1.500  \n",
      "Step:3050 | Total games:58 | Mean reward: 1.700  \n",
      "Step:3088 | Total games:59 | Mean reward: 1.500  \n",
      "Step:3147 | Total games:60 | Mean reward: 1.800  \n",
      "Step:3183 | Total games:61 | Mean reward: 1.400  \n",
      "Step:3228 | Total games:62 | Mean reward: 1.300  \n",
      "Step:3284 | Total games:63 | Mean reward: 1.200  \n",
      "Step:3319 | Total games:64 | Mean reward: 1.200  \n",
      "Step:3361 | Total games:65 | Mean reward: 1.100  \n",
      "Step:3408 | Total games:66 | Mean reward: 1.100  \n",
      "Step:3476 | Total games:67 | Mean reward: 1.300  \n",
      "Step:3518 | Total games:68 | Mean reward: 1.200  \n",
      "Step:3578 | Total games:69 | Mean reward: 1.400  \n",
      "Step:3632 | Total games:70 | Mean reward: 1.200  \n",
      "Step:3670 | Total games:71 | Mean reward: 1.300  \n",
      "Step:3746 | Total games:72 | Mean reward: 1.600  \n",
      "Step:3787 | Total games:73 | Mean reward: 1.600  \n",
      "Step:3839 | Total games:74 | Mean reward: 1.800  \n",
      "Step:3881 | Total games:75 | Mean reward: 1.800  \n",
      "Step:3954 | Total games:76 | Mean reward: 2.100  \n",
      "Step:4001 | Total games:77 | Mean reward: 1.800  \n",
      "Step:4043 | Total games:78 | Mean reward: 1.700  \n",
      "Step:4079 | Total games:79 | Mean reward: 1.500  \n",
      "Step:4134 | Total games:80 | Mean reward: 1.500  \n",
      "Step:4170 | Total games:81 | Mean reward: 1.400  \n",
      "Step:4204 | Total games:82 | Mean reward: 1.000  \n",
      "Step:4254 | Total games:83 | Mean reward: 1.100  \n",
      "Step:4319 | Total games:84 | Mean reward: 1.200  \n",
      "Step:4427 | Total games:85 | Mean reward: 1.800  \n",
      "Step:4456 | Total games:86 | Mean reward: 1.400  \n",
      "Step:4498 | Total games:87 | Mean reward: 1.400  \n",
      "Step:4543 | Total games:88 | Mean reward: 1.500  \n",
      "Step:4601 | Total games:89 | Mean reward: 1.700  \n",
      "Step:4651 | Total games:90 | Mean reward: 1.800  \n",
      "Step:4704 | Total games:91 | Mean reward: 2.000  \n",
      "Step:4758 | Total games:92 | Mean reward: 2.100  \n",
      "Step:4814 | Total games:93 | Mean reward: 2.100  \n",
      "Step:4847 | Total games:94 | Mean reward: 1.800  \n",
      "Step:4901 | Total games:95 | Mean reward: 1.300  \n",
      "Step:4978 | Total games:96 | Mean reward: 1.700  \n",
      "Step:5027 | Total games:97 | Mean reward: 1.800  \n",
      "Step:5091 | Total games:98 | Mean reward: 2.000  \n",
      "Step:5123 | Total games:99 | Mean reward: 1.800  \n",
      "Step:5181 | Total games:100 | Mean reward: 1.800  \n",
      "Step:5233 | Total games:101 | Mean reward: 1.700  \n",
      "Step:5266 | Total games:102 | Mean reward: 1.600  \n",
      "Step:5321 | Total games:103 | Mean reward: 1.600  \n",
      "Step:5429 | Total games:104 | Mean reward: 2.400  \n",
      "Step:5497 | Total games:105 | Mean reward: 2.500  \n",
      "Step:5569 | Total games:106 | Mean reward: 2.400  \n",
      "Step:5626 | Total games:107 | Mean reward: 2.400  \n",
      "Step:5671 | Total games:108 | Mean reward: 2.200  \n",
      "Step:5747 | Total games:109 | Mean reward: 2.600  \n",
      "Step:5791 | Total games:110 | Mean reward: 2.500  \n",
      "Step:5852 | Total games:111 | Mean reward: 2.700  \n",
      "Step:5892 | Total games:112 | Mean reward: 2.800  \n",
      "Step:5933 | Total games:113 | Mean reward: 2.700  \n",
      "Step:5969 | Total games:114 | Mean reward: 1.900  \n",
      "Step:6013 | Total games:115 | Mean reward: 1.700  \n",
      "Step:6082 | Total games:116 | Mean reward: 1.800  \n",
      "Step:6129 | Total games:117 | Mean reward: 1.700  \n",
      "Step:6165 | Total games:118 | Mean reward: 1.600  \n",
      "Step:6233 | Total games:119 | Mean reward: 1.500  \n",
      "Step:6289 | Total games:120 | Mean reward: 1.700  \n",
      "Step:6321 | Total games:121 | Mean reward: 1.400  \n",
      "Step:6362 | Total games:122 | Mean reward: 1.300  \n",
      "Step:6419 | Total games:123 | Mean reward: 1.500  \n",
      "Step:6458 | Total games:124 | Mean reward: 1.600  \n",
      "Step:6512 | Total games:125 | Mean reward: 1.600  \n",
      "Step:6551 | Total games:126 | Mean reward: 1.200  \n",
      "Step:6583 | Total games:127 | Mean reward: 1.100  \n",
      "Step:6624 | Total games:128 | Mean reward: 1.100  \n",
      "Step:6659 | Total games:129 | Mean reward: 0.800  \n",
      "Step:6696 | Total games:130 | Mean reward: 0.500  \n",
      "Step:6745 | Total games:131 | Mean reward: 0.700  \n",
      "Step:6789 | Total games:132 | Mean reward: 0.700  \n",
      "Step:6827 | Total games:133 | Mean reward: 0.400  \n",
      "Step:6866 | Total games:134 | Mean reward: 0.300  \n",
      "Step:6921 | Total games:135 | Mean reward: 0.400  \n",
      "Step:6999 | Total games:136 | Mean reward: 0.900  \n",
      "Step:7053 | Total games:137 | Mean reward: 1.100  \n",
      "Step:7106 | Total games:138 | Mean reward: 1.300  \n",
      "Step:7149 | Total games:139 | Mean reward: 1.400  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m target_net\u001b[38;5;241m.\u001b[39mreset_noise()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#if(step_number % 2 == 0): epsilon = max(epsilon * EPS_DECAY, EPS_MIN)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Here we update beta from INITIAL_BETA to 1.0\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#buffer.beta = min(1.0, buffer.beta + (1.0 - INITIAL_BETA) / 500000) # This is divided by 500000 because this is the expected number of steps for the algorithm to run. In this way, beta will be 1.0 when the algorithm reach the 500000 steps\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m reward \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mstep(net, epsilon, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m#Hacer un reward por episodio real y luego meterlo en la lista de total_rewards como un solo nÃºmero\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#tmp_reward_per_episode+=reward\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#if(env.was_real_done): #Real end of the episode\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m#total_rewards.append(tmp_reward_per_episode)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     total_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mDQNAgent.step\u001b[1;34m(self, net, epsilon, device)\u001b[0m\n\u001b[0;32m     19\u001b[0m _, act_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(q_vals, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(act_\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 22\u001b[0m new_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     23\u001b[0m is_done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:416\u001b[0m, in \u001b[0;36mFrameStackObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and info from the environment\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m     obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[0;32m    419\u001b[0m     updated_obs \u001b[38;5;241m=\u001b[39m deepcopy(\n\u001b[0;32m    420\u001b[0m         concatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_queue, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacked_obs)\n\u001b[0;32m    421\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "    \u001b[1;31m[... skipping similar frames: ObservationWrapper.step at line 550 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\core.py:550\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 550\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36mFireResetEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\stateful_observation.py:618\u001b[0m, in \u001b[0;36mMaxAndSkipObservation.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 618\u001b[0m max_frame \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m max_frame, total_reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2810\u001b[0m, in \u001b[0;36mmax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[0;32m   2695\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[0;32m   2696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2697\u001b[0m \u001b[38;5;124;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[0;32m   2698\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2808\u001b[0m \u001b[38;5;124;03m    5\u001b[39;00m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mmaximum, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   2811\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, initial\u001b[38;5;241m=\u001b[39minitial, where\u001b[38;5;241m=\u001b[39mwhere)\n",
      "File \u001b[1;32mc:\\Users\\arroc\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = NoisyDuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_net = NoisyDuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "buffer = ExperienceReplay(EXPERIENCE_REPLAY_SIZE)\n",
    "agent = DQNAgent(env, buffer)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "losses = []\n",
    "step_number = 0\n",
    "spected_min_reward=30\n",
    "\n",
    "\n",
    "while True:\n",
    "    step_number += 1\n",
    "\n",
    "    # Reset noise after every training step\n",
    "    net.reset_noise()\n",
    "    target_net.reset_noise()\n",
    "\n",
    "    reward = agent.step(net, device=device)\n",
    "    if reward is not None:\n",
    "\n",
    "        total_rewards.append(reward)\n",
    "\n",
    "        mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "        \n",
    "        print(f\"Step:{step_number} | Total games:{len(total_rewards)} | Mean reward: {mean_reward:.3f}  \") \n",
    "        wandb.log({\"reward_mean\": mean_reward, \"reward\": reward}, step=step_number)\n",
    "        \n",
    "        # Every time we achieve a spected reward we save the current model, and we save the next every time the mean reward increases 5 points \n",
    "        if mean_reward > spected_min_reward:\n",
    "            name=\"Part1_DQN_\"+str(int(mean_reward))\n",
    "            torch.save(net.state_dict(), f\"{name}.dat\")\n",
    "            print(\"New best model saved.\")\n",
    "            spected_min_reward +=5\n",
    "\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(f\"SOLVED in {step_number} steps and {len(total_rewards)} games\")\n",
    "            break\n",
    "\n",
    "    if len(buffer) < EXPERIENCE_REPLAY_SIZE:\n",
    "        continue\n",
    "\n",
    "    states_, actions_, rewards_, dones_, next_states_ = buffer.sample(BATCH_SIZE)\n",
    "\n",
    "    states = torch.tensor(states_).to(device)\n",
    "    next_states = torch.tensor(next_states_).to(device)\n",
    "    actions = torch.tensor(actions_).to(device)\n",
    "    rewards = torch.tensor(rewards_).to(device)\n",
    "    dones = torch.BoolTensor(dones_).to(device)\n",
    "\n",
    "    Q_values = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # Here we get the actions selected by the policy network\n",
    "    policy_actions = net(next_states).argmax(1).unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "\n",
    "    # Here we use the target network to compute the value of those actions\n",
    "    next_state_values = target_net(next_states).gather(1, policy_actions).squeeze(1) # Doble DQN\n",
    "    next_state_values[dones] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_Q_values = next_state_values * GAMMA + rewards\n",
    "\n",
    "    loss = nn.MSELoss()(Q_values, expected_Q_values)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    mean_losses = np.mean(losses[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "    wandb.log({\"loss_mean\": mean_losses, \"loss\": loss.item()}, step=step_number)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step_number % SYNC_TARGET_NETWORK == 0:\n",
    "        target_net.load_state_dict(net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T19:35:09.784670Z",
     "iopub.status.idle": "2024-12-05T19:35:09.785014Z",
     "shell.execute_reply": "2024-12-05T19:35:09.784868Z",
     "shell.execute_reply.started": "2024-12-05T19:35:09.784851Z"
    },
    "id": "FMIpE4pRs1Py",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"Part1_DQN.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T19:35:09.785920Z",
     "iopub.status.idle": "2024-12-05T19:35:09.786237Z",
     "shell.execute_reply": "2024-12-05T19:35:09.786100Z",
     "shell.execute_reply.started": "2024-12-05T19:35:09.786083Z"
    },
    "id": "u-8blrvas1Py",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\">>> Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T19:35:09.787501Z",
     "iopub.status.idle": "2024-12-05T19:35:09.787837Z",
     "shell.execute_reply": "2024-12-05T19:35:09.787691Z",
     "shell.execute_reply.started": "2024-12-05T19:35:09.787673Z"
    },
    "id": "1BbjwiX1s1Py",
    "outputId": "54ea7229-8dcd-4955-9ed1-c5d46fae47ec",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4267572eef044addb342263acfcf13ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.020 MB of 0.020 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>reward</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr><tr><td>reward_mean</td><td>ââââââââââââââââââââââââââââââââââââââââ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>reward</td><td>1</td></tr><tr><td>reward_mean</td><td>1.4</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-breeze-43</strong> at: <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN/runs/8jwwxw0a</a><br/> View project at: <a href='https://wandb.ai/arroch35-organitzation/Part1_DQN' target=\"_blank\">https://wandb.ai/arroch35-organitzation/Part1_DQN</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241206_140002-8jwwxw0a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Finish the wandb run, necessary in notebooks\n",
    "wandb.finish()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T19:35:09.789400Z",
     "iopub.status.idle": "2024-12-05T19:35:09.789874Z",
     "shell.execute_reply": "2024-12-05T19:35:09.789657Z",
     "shell.execute_reply.started": "2024-12-05T19:35:09.789631Z"
    },
    "id": "X8LvND3N_c7S",
    "outputId": "ed9356a4-f6f2-4dc9-9276-34dceaece4a3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arroc\\AppData\\Local\\Temp\\ipykernel_12348\\537343770.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../models/Part1_DQN_35.dat\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NoisyDuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "model.load_state_dict(torch.load(\"Part1_DQN_35.dat\", map_location=torch.device(device))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to test the performance of the agent. \n",
    "# A part from this, it also saves the best episode to later generate a video\n",
    "def test_agent(model, env, num_episodes=100):\n",
    "    total_rewards = []\n",
    "    \n",
    "    best_reward=30\n",
    "    best_episode=[]\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        images=[]\n",
    "        while not done:\n",
    "            img = env.render()\n",
    "            images.append(Image.fromarray(img))\n",
    "            state_ = torch.tensor(np.array([state], copy=False)).to(device)\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(state_).data.cpu().numpy()[0]\n",
    "            action = np.argmax(q_vals)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_reward+=reward\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # If the current episode's reward exceeds the best reward so far, update the best episode\n",
    "        if(episode_reward>best_reward):\n",
    "            best_episode=images\n",
    "            best_reward=episode_reward\n",
    "            \n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\" Episode: {episode} | Total reward: {episode_reward:.3f}\")\n",
    "\n",
    "    avg_reward = sum(total_rewards) / num_episodes\n",
    "    print(f\"Test Results: Average Reward over {num_episodes} episodes: {avg_reward:.3f}\")\n",
    "\n",
    "    return best_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode: 0 | Total reward: 14.000\n",
      " Episode: 1 | Total reward: 20.000\n",
      " Episode: 2 | Total reward: 32.000\n",
      " Episode: 3 | Total reward: 30.000\n",
      " Episode: 4 | Total reward: 17.000\n",
      " Episode: 5 | Total reward: 37.000\n",
      " Episode: 6 | Total reward: 34.000\n",
      " Episode: 7 | Total reward: 50.000\n",
      " Episode: 8 | Total reward: 18.000\n",
      " Episode: 9 | Total reward: 36.000\n",
      " Episode: 10 | Total reward: 14.000\n",
      " Episode: 11 | Total reward: 17.000\n",
      " Episode: 12 | Total reward: 24.000\n",
      " Episode: 13 | Total reward: 26.000\n",
      " Episode: 14 | Total reward: 32.000\n",
      " Episode: 15 | Total reward: 18.000\n",
      " Episode: 16 | Total reward: 23.000\n",
      " Episode: 17 | Total reward: 41.000\n",
      " Episode: 18 | Total reward: 28.000\n",
      " Episode: 19 | Total reward: 11.000\n",
      " Episode: 20 | Total reward: 27.000\n",
      " Episode: 21 | Total reward: 31.000\n",
      " Episode: 22 | Total reward: 39.000\n",
      " Episode: 23 | Total reward: 24.000\n",
      " Episode: 24 | Total reward: 16.000\n",
      " Episode: 25 | Total reward: 30.000\n",
      " Episode: 26 | Total reward: 19.000\n",
      " Episode: 27 | Total reward: 33.000\n",
      " Episode: 28 | Total reward: 28.000\n",
      " Episode: 29 | Total reward: 35.000\n",
      " Episode: 30 | Total reward: 33.000\n",
      " Episode: 31 | Total reward: 23.000\n",
      " Episode: 32 | Total reward: 25.000\n",
      " Episode: 33 | Total reward: 36.000\n",
      " Episode: 34 | Total reward: 28.000\n",
      " Episode: 35 | Total reward: 33.000\n",
      " Episode: 36 | Total reward: 13.000\n",
      " Episode: 37 | Total reward: 32.000\n",
      " Episode: 38 | Total reward: 29.000\n",
      " Episode: 39 | Total reward: 15.000\n",
      " Episode: 40 | Total reward: 14.000\n",
      " Episode: 41 | Total reward: 30.000\n",
      " Episode: 42 | Total reward: 21.000\n",
      " Episode: 43 | Total reward: 29.000\n",
      " Episode: 44 | Total reward: 40.000\n",
      " Episode: 45 | Total reward: 4.000\n",
      " Episode: 46 | Total reward: 15.000\n",
      " Episode: 47 | Total reward: 26.000\n",
      " Episode: 48 | Total reward: 19.000\n",
      " Episode: 49 | Total reward: 16.000\n",
      " Episode: 50 | Total reward: 48.000\n",
      " Episode: 51 | Total reward: 35.000\n",
      " Episode: 52 | Total reward: 30.000\n",
      " Episode: 53 | Total reward: 24.000\n",
      " Episode: 54 | Total reward: 40.000\n",
      " Episode: 55 | Total reward: 36.000\n",
      " Episode: 56 | Total reward: 31.000\n",
      " Episode: 57 | Total reward: 37.000\n",
      " Episode: 58 | Total reward: 40.000\n",
      " Episode: 59 | Total reward: 24.000\n",
      " Episode: 60 | Total reward: 28.000\n",
      " Episode: 61 | Total reward: 25.000\n",
      " Episode: 62 | Total reward: 15.000\n",
      " Episode: 63 | Total reward: 48.000\n",
      " Episode: 64 | Total reward: 29.000\n",
      " Episode: 65 | Total reward: 34.000\n",
      " Episode: 66 | Total reward: 34.000\n",
      " Episode: 67 | Total reward: 26.000\n",
      " Episode: 68 | Total reward: 16.000\n",
      " Episode: 69 | Total reward: 22.000\n",
      " Episode: 70 | Total reward: 25.000\n",
      " Episode: 71 | Total reward: 42.000\n",
      " Episode: 72 | Total reward: 26.000\n",
      " Episode: 73 | Total reward: 31.000\n",
      " Episode: 74 | Total reward: 24.000\n",
      " Episode: 75 | Total reward: 28.000\n",
      " Episode: 76 | Total reward: 15.000\n",
      " Episode: 77 | Total reward: 23.000\n",
      " Episode: 78 | Total reward: 34.000\n",
      " Episode: 79 | Total reward: 35.000\n",
      " Episode: 80 | Total reward: 23.000\n",
      " Episode: 81 | Total reward: 30.000\n",
      " Episode: 82 | Total reward: 28.000\n",
      " Episode: 83 | Total reward: 32.000\n",
      " Episode: 84 | Total reward: 35.000\n",
      " Episode: 85 | Total reward: 18.000\n",
      " Episode: 86 | Total reward: 19.000\n",
      " Episode: 87 | Total reward: 25.000\n",
      " Episode: 88 | Total reward: 22.000\n",
      " Episode: 89 | Total reward: 26.000\n",
      " Episode: 90 | Total reward: 34.000\n",
      " Episode: 91 | Total reward: 25.000\n",
      " Episode: 92 | Total reward: 29.000\n",
      " Episode: 93 | Total reward: 28.000\n",
      " Episode: 94 | Total reward: 31.000\n",
      " Episode: 95 | Total reward: 11.000\n",
      " Episode: 96 | Total reward: 18.000\n",
      " Episode: 97 | Total reward: 23.000\n",
      " Episode: 98 | Total reward: 24.000\n",
      " Episode: 99 | Total reward: 12.000\n",
      "Test Results: Average Reward over 100 episodes: 26.830\n"
     ]
    }
   ],
   "source": [
    "images=test_agent(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-05T19:35:09.793680Z",
     "iopub.status.idle": "2024-12-05T19:35:09.794153Z",
     "shell.execute_reply": "2024-12-05T19:35:09.793932Z",
     "shell.execute_reply.started": "2024-12-05T19:35:09.793892Z"
    },
    "id": "6LOTlTvT_c7T",
    "outputId": "45fcf855-1da6-44cd-965a-0f5b1cc07d8b",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode export to '../data/video_noisy4.gif'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gif_file = \"video_noisy4.gif\"\n",
    "\n",
    "images[0].save(gif_file, save_all=True, append_images=images[1:], duration=160, loop=0)\n",
    "\n",
    "print(\"Episode export to '{}'\".format(gif_file))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
