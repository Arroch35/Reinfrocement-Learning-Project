{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: BREAKOUT WITH REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dependencies (we used Kaggle for this part)\n",
    "!pip install gymnasium==1.0.0\n",
    "!pip install ale-py\n",
    "!pip install wandb\n",
    "!pip install torchsummary\"\"\" \n",
    "\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchsummary import summary\n",
    "\n",
    "import collections\n",
    "\n",
    "import wandb\n",
    "import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# version\n",
    "print(\"Using Gymnasium version {}\".format(gym.__version__))\n",
    "\n",
    "ENV_NAME = \"ALE/Breakout-v5\"\n",
    "test_env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "\n",
    "print(test_env.unwrapped.get_action_meanings())\n",
    "print(test_env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Source: M3-2_Example_1a (DQN on Pong, train)\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "    env = MaxAndSkipObservation(env, skip=4)\n",
    "    print(\"MaxAndSkipObservation: {}\".format(env.observation_space.shape))\n",
    "    #env = FireResetEnv(env)\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "    env = GrayscaleObservation(env, keep_dim=True)\n",
    "    print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n",
    "    env = ImageToPyTorch(env)\n",
    "    print(\"ImageToPyTorch       : {}\".format(env.observation_space.shape))\n",
    "    env = ReshapeObservation(env, (84, 84))\n",
    "    print(\"ReshapeObservation   : {}\".format(env.observation_space.shape))\n",
    "    env = FrameStackObservation(env, stack_size=4)\n",
    "    print(\"FrameStackObservation: {}\".format(env.observation_space.shape))\n",
    "    env = ScaledFloatFrame(env)\n",
    "    print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "\n",
    "    return env\n",
    "\n",
    "env=make_env(ENV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLICY NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.zeros((1, *input_shape))\n",
    "            feature_size = self.feature_extractor(sample_input).shape[1]\n",
    "\n",
    "        # Policy head (action logits)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_actions)\n",
    "        )\n",
    "\n",
    "        # Value head (baseline)\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        action_logits = self.policy(features)\n",
    "        state_value = self.value(features)\n",
    "        return action_logits, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env, device, learning_rate=1e-3, gamma=0.99, value_loss_coeff=0.5):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.value_loss_coeff = value_loss_coeff\n",
    "\n",
    "        self.policy_net = PolicyNetwork(env.observation_space.shape, env.action_space.n).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        self.saved_values = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        logits, state_value = self.policy_net(state)\n",
    "        action_probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        self.saved_values.append(state_value)\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def finish_episode(self):\n",
    "        # Calculate losses\n",
    "        R = 0\n",
    "        returns = []\n",
    "        for r in self.rewards[::-1]:\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns).to(self.device)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "\n",
    "        for log_prob, value, R in zip(self.saved_log_probs, self.saved_values, returns):\n",
    "            # Calculate advantage\n",
    "            advantage = R - value.detach()\n",
    "            policy_losses.append(-log_prob * advantage)\n",
    "            value_losses.append(nn.MSELoss()(value.squeeze(), torch.tensor([[R]]).to(self.device)))  # Ensure target is of shape (1, 1)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_policy_loss = torch.stack(policy_losses).sum()\n",
    "        total_value_loss = torch.stack(value_losses).sum()\n",
    "        total_loss = total_policy_loss + self.value_loss_coeff * total_value_loss\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        del self.rewards[:]\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_values[:]\n",
    "\n",
    "        return total_policy_loss.item(), total_value_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_EPISODES = 100000\n",
    "NUMBER_OF_REWARDS_TO_AVERAGE = 10\n",
    "GAMMA = 0.995\n",
    "LEARNING_RATE = 1e-3\n",
    "VALUE_LOSS_COEFF = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    wandb.login(key=\"YOUR_API_KEY\")\n",
    "    \n",
    "    wandb.init(project=\"breakout-reinforce\", config={\n",
    "        \"gamma\": GAMMA,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"value_loss_coeff\": VALUE_LOSS_COEFF,\n",
    "    })\n",
    "    \n",
    "    env = make_env(ENV_NAME)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    agent = REINFORCEAgent(env, device, learning_rate=LEARNING_RATE, gamma=GAMMA, value_loss_coeff=VALUE_LOSS_COEFF)\n",
    "    \n",
    "    total_rewards = []\n",
    "    best_mean_reward = None\n",
    "    \n",
    "    for episode in range(MAX_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0 \n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                total_rewards.append(episode_reward)\n",
    "                mean_reward = np.mean(total_rewards[-NUMBER_OF_REWARDS_TO_AVERAGE:])\n",
    "                \n",
    "                policy_loss, value_loss = agent.finish_episode()\n",
    "                \n",
    "                # Log metrics with WandB\n",
    "                wandb.log({\n",
    "                    \"episode\": episode,\n",
    "                    \"reward\": episode_reward,\n",
    "                    \"mean_reward\": mean_reward,\n",
    "                    \"policy_loss\": policy_loss,\n",
    "                    \"value_loss\": value_loss,\n",
    "                    \"steps_per_episode\": steps\n",
    "                })\n",
    "                \n",
    "                # Save the best-performing model\n",
    "                if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                    torch.save(agent.policy_net.state_dict(), \"../../models/breakout/REINFORCE_policy_net.dat\")\n",
    "                    best_mean_reward = mean_reward\n",
    "                \n",
    "                print(f\"Episode {episode}, reward: {episode_reward:.2f}, mean reward: {mean_reward:.2f}\")\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearch for Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"from itertools import product\n",
    "\n",
    "GAMMA_VALUES = [0.95, 0.99, 0.999]\n",
    "LEARNING_RATE_VALUES = [1e-3, 1e-4, 1e-5]\n",
    "VALUE_LOSS_COEFF_VALUES = [0.1, 0.5, 1.0]\n",
    "\n",
    "def train_with_hyperparameters():\n",
    "    for gamma, learning_rate, value_loss_coeff in product(GAMMA_VALUES, LEARNING_RATE_VALUES, VALUE_LOSS_COEFF_VALUES):\n",
    "        print(f\"Training with GAMMA={gamma}, LEARNING_RATE={learning_rate}, VALUE_LOSS_COEFF={value_loss_coeff}\")\n",
    "        \n",
    "        wandb.init(\n",
    "            project=\"breakout-reinforce-hyperparam-search\",\n",
    "            config={\n",
    "                \"gamma\": gamma,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"value_loss_coeff\": value_loss_coeff,\n",
    "            },\n",
    "            reinit=True  # Allow multiple runs in the same script\n",
    "        )\n",
    "\n",
    "        env = make_env(ENV_NAME)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        agent = REINFORCEAgent(env, device, learning_rate=learning_rate, gamma=gamma, value_loss_coeff=value_loss_coeff)\n",
    "\n",
    "        total_rewards = []\n",
    "        best_mean_reward = None\n",
    "\n",
    "        for episode in range(2000): \n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            while True:\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                agent.rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "\n",
    "                if done:\n",
    "                    total_rewards.append(episode_reward)\n",
    "                    mean_reward = np.mean(total_rewards[-10:]) \n",
    "\n",
    "                    policy_loss, value_loss = agent.finish_episode()\n",
    "\n",
    "                    # Log metrics with WandB\n",
    "                    wandb.log({\n",
    "                        \"episode\": episode,\n",
    "                        \"reward\": episode_reward,\n",
    "                        \"mean_reward\": mean_reward,\n",
    "                        \"policy_loss\": policy_loss,\n",
    "                        \"value_loss\": value_loss,\n",
    "                        \"steps_per_episode\": steps,\n",
    "                    })\n",
    "\n",
    "                    # Save the best-performing model\n",
    "                    if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                        model_name = f\"policy_net_gamma{gamma}_lr{learning_rate}_vlc{value_loss_coeff}.dat\"\n",
    "                        torch.save(agent.policy_net.state_dict(), f\"/kaggle/working/{model_name}\")\n",
    "                        best_mean_reward = mean_reward\n",
    "\n",
    "                    print(f\"Ep {episode}, reward: {episode_reward:.2f}, mean reward: {mean_reward:.2f}, \"\n",
    "                          f\"gamma={gamma}, lr={learning_rate}, vlc={value_loss_coeff}\")\n",
    "\n",
    "                    break\n",
    "\n",
    "        # Log final results for the current combination\n",
    "        print(f\"Completed training for GAMMA={gamma}, LEARNING_RATE={learning_rate}, VALUE_LOSS_COEFF={value_loss_coeff}\")\n",
    "        wandb.finish()\n",
    "\n",
    "# Main Body\n",
    "print(\"Training starts at\", datetime.datetime.now())\n",
    "train_with_hyperparameters()\n",
    "print(\"Training ends at\", datetime.datetime.now())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN BODY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Training starts at\", datetime.datetime.now())\n",
    "train()\n",
    "print(\"Training ends at\", datetime.datetime.now())\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "policy_net.load_state_dict(torch.load(\"model_REINFORCE.dat\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "policy_net.eval() #Remove dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKING GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "visualize = True\n",
    "images = []\n",
    "gif_file = \"video_REINFORCE.gif\"\n",
    "\n",
    "# Reset environment\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "# Play one episode\n",
    "while True:\n",
    "    start_ts = time.time()\n",
    "\n",
    "    if visualize:\n",
    "        # Render the environment's frame (for RGB environments)\n",
    "        img = env.render()\n",
    "        images.append(Image.fromarray(img))  # Store for GIF creation\n",
    "\n",
    "    # Convert state to tensor and get the action from the policy network\n",
    "    state_tensor = torch.tensor(np.array([state], copy=False)).float().to(device)\n",
    "    logits, _ = policy_net(state_tensor)  # Assuming policy_net returns action logits and state value\n",
    "    action_probs = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    action = torch.multinomial(action_probs, 1).item()  # Sample action from probability distribution\n",
    "\n",
    "    # Step in the environment\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Total reward: {total_reward:.2f}\")\n",
    "\n",
    "# Create GIF from the frames collected\n",
    "images[0].save(f\"../../videos/breakout/{gif_file}\", save_all=True, append_images=images[1:], duration=60, loop=0)\n",
    "print(f\"Episode exported to ../../videos/breakout/{gif_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
