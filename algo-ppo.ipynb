{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# pip install --upgrade kaggle-environments","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:03.850748Z","iopub.execute_input":"2024-12-11T21:51:03.851191Z","iopub.status.idle":"2024-12-11T21:51:04.222014Z","shell.execute_reply.started":"2024-12-11T21:51:03.851155Z","shell.execute_reply":"2024-12-11T21:51:04.221137Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# # imports\n# !pip install stable-baselines3 pyglet imageio-ffmpeg gymnasium[atari,accept-rom-license] ale-py > /dev/null 2>&1\n# !pip install gymnasium==1.0.0\n!pip install gymnasium==0.29.1\n!pip install ale_py\n!pip install stable-baselines3\n# !pip install stable-baselines3[extra]\n# pip install sb3-contrib","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:04.223609Z","iopub.execute_input":"2024-12-11T21:51:04.224275Z","iopub.status.idle":"2024-12-11T21:51:29.985983Z","shell.execute_reply.started":"2024-12-11T21:51:04.224232Z","shell.execute_reply":"2024-12-11T21:51:29.985268Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium==0.29.1 in /opt/conda/lib/python3.10/site-packages (0.29.1)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==0.29.1) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==0.29.1) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==0.29.1) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium==0.29.1) (0.0.4)\nRequirement already satisfied: ale_py in /opt/conda/lib/python3.10/site-packages (0.10.1)\nRequirement already satisfied: numpy>1.20 in /opt/conda/lib/python3.10/site-packages (from ale_py) (1.26.4)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale_py) (4.12.2)\nRequirement already satisfied: stable-baselines3 in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (0.29.1)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (1.26.4)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.4.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.1.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (2.2.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3) (3.7.5)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3) (2024.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:29.987266Z","iopub.execute_input":"2024-12-11T21:51:29.987626Z","iopub.status.idle":"2024-12-11T21:51:30.238300Z","shell.execute_reply.started":"2024-12-11T21:51:29.987590Z","shell.execute_reply":"2024-12-11T21:51:30.237645Z"},"id":"Nv2c8G-fJni_","trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"import ale_py","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:30.240667Z","iopub.execute_input":"2024-12-11T21:51:30.241022Z","iopub.status.idle":"2024-12-11T21:51:30.564936Z","shell.execute_reply.started":"2024-12-11T21:51:30.240982Z","shell.execute_reply":"2024-12-11T21:51:30.564271Z"},"id":"vvF3dyK5JnjA","trusted":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\nfrom stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\nimport torch\nimport numpy as np\nimport wandb\nfrom wandb.integration.sb3 import WandbCallback\n\nfrom gymnasium.wrappers import ResizeObservation","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:30.565914Z","iopub.execute_input":"2024-12-11T21:51:30.566177Z","iopub.status.idle":"2024-12-11T21:51:30.813218Z","shell.execute_reply.started":"2024-12-11T21:51:30.566152Z","shell.execute_reply":"2024-12-11T21:51:30.812511Z"},"id":"p--B9ZV5JnjA","trusted":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"print(gym.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:30.814354Z","iopub.execute_input":"2024-12-11T21:51:30.814692Z","iopub.status.idle":"2024-12-11T21:51:31.089774Z","shell.execute_reply.started":"2024-12-11T21:51:30.814654Z","shell.execute_reply":"2024-12-11T21:51:31.089104Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0.29.1\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"from stable_baselines3.common.monitor import Monitor","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:31.090686Z","iopub.execute_input":"2024-12-11T21:51:31.090961Z","iopub.status.idle":"2024-12-11T21:51:31.378592Z","shell.execute_reply.started":"2024-12-11T21:51:31.090934Z","shell.execute_reply":"2024-12-11T21:51:31.377919Z"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1733279365196,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"GOiINwN2f1aQ","outputId":"8bfe846d-efaa-4cd7-8933-99a323884c98","trusted":true},"outputs":[],"execution_count":54},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:31.379706Z","iopub.execute_input":"2024-12-11T21:51:31.380062Z","iopub.status.idle":"2024-12-11T21:51:31.649144Z","shell.execute_reply.started":"2024-12-11T21:51:31.380024Z","shell.execute_reply":"2024-12-11T21:51:31.648275Z"},"id":"Uujp09wlJnjB","trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"import os","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:31.650168Z","iopub.execute_input":"2024-12-11T21:51:31.650424Z","iopub.status.idle":"2024-12-11T21:51:31.874196Z","shell.execute_reply.started":"2024-12-11T21:51:31.650398Z","shell.execute_reply":"2024-12-11T21:51:31.873572Z"},"id":"wu-A96lcJnjB","trusted":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"from stable_baselines3 import DQN\nfrom stable_baselines3.common.callbacks import CheckpointCallback","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:31.876649Z","iopub.execute_input":"2024-12-11T21:51:31.876900Z","iopub.status.idle":"2024-12-11T21:51:32.076620Z","shell.execute_reply.started":"2024-12-11T21:51:31.876857Z","shell.execute_reply":"2024-12-11T21:51:32.075998Z"},"id":"WJUt4ja8JnjB","trusted":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":"gym.register_envs(ale_py)","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:32.077505Z","iopub.execute_input":"2024-12-11T21:51:32.077729Z","iopub.status.idle":"2024-12-11T21:51:32.321316Z","shell.execute_reply.started":"2024-12-11T21:51:32.077706Z","shell.execute_reply":"2024-12-11T21:51:32.320624Z"},"id":"3qpLc3uJJnjB","trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# from sb3_contrib import QRDQN","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:32.322377Z","iopub.execute_input":"2024-12-11T21:51:32.322658Z","iopub.status.idle":"2024-12-11T21:51:32.581509Z","shell.execute_reply.started":"2024-12-11T21:51:32.322630Z","shell.execute_reply":"2024-12-11T21:51:32.580894Z"},"id":"pfzpW-NAJnjB","trusted":true},"outputs":[],"execution_count":59},{"cell_type":"code","source":"from datetime import datetime\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.ppo.policies import MlpPolicy\nfrom wandb.integration.sb3 import WandbCallback","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:32.582497Z","iopub.execute_input":"2024-12-11T21:51:32.582833Z","iopub.status.idle":"2024-12-11T21:51:32.831709Z","shell.execute_reply.started":"2024-12-11T21:51:32.582796Z","shell.execute_reply":"2024-12-11T21:51:32.831074Z"},"id":"S4uVRdT9f1aS","trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"from stable_baselines3.common.env_util import make_vec_env","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:32.832746Z","iopub.execute_input":"2024-12-11T21:51:32.833080Z","iopub.status.idle":"2024-12-11T21:51:33.041300Z","shell.execute_reply.started":"2024-12-11T21:51:32.833044Z","shell.execute_reply":"2024-12-11T21:51:33.040578Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"from stable_baselines3 import PPO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:33.042365Z","iopub.execute_input":"2024-12-11T21:51:33.042648Z","iopub.status.idle":"2024-12-11T21:51:33.294077Z","shell.execute_reply.started":"2024-12-11T21:51:33.042620Z","shell.execute_reply":"2024-12-11T21:51:33.293315Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import EvalCallback, CallbackList","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:33.295117Z","iopub.execute_input":"2024-12-11T21:51:33.295407Z","iopub.status.idle":"2024-12-11T21:51:33.503427Z","shell.execute_reply.started":"2024-12-11T21:51:33.295378Z","shell.execute_reply":"2024-12-11T21:51:33.502610Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"import collections","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:33.504584Z","iopub.execute_input":"2024-12-11T21:51:33.505239Z","iopub.status.idle":"2024-12-11T21:51:33.766352Z","shell.execute_reply.started":"2024-12-11T21:51:33.505197Z","shell.execute_reply":"2024-12-11T21:51:33.765698Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:33.767236Z","iopub.execute_input":"2024-12-11T21:51:33.767481Z","iopub.status.idle":"2024-12-11T21:51:33.993422Z","shell.execute_reply.started":"2024-12-11T21:51:33.767455Z","shell.execute_reply":"2024-12-11T21:51:33.992809Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import VecEnvWrapper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:33.994286Z","iopub.execute_input":"2024-12-11T21:51:33.994532Z","iopub.status.idle":"2024-12-11T21:51:34.275256Z","shell.execute_reply.started":"2024-12-11T21:51:33.994508Z","shell.execute_reply":"2024-12-11T21:51:34.274633Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:34.276110Z","iopub.execute_input":"2024-12-11T21:51:34.276348Z","iopub.status.idle":"2024-12-11T21:51:34.470141Z","shell.execute_reply.started":"2024-12-11T21:51:34.276323Z","shell.execute_reply":"2024-12-11T21:51:34.469542Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"from gymnasium.spaces import Discrete","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:34.471119Z","iopub.execute_input":"2024-12-11T21:51:34.471459Z","iopub.status.idle":"2024-12-11T21:51:34.674201Z","shell.execute_reply.started":"2024-12-11T21:51:34.471416Z","shell.execute_reply":"2024-12-11T21:51:34.673576Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:34.675191Z","iopub.execute_input":"2024-12-11T21:51:34.675461Z","iopub.status.idle":"2024-12-11T21:51:34.895502Z","shell.execute_reply.started":"2024-12-11T21:51:34.675417Z","shell.execute_reply":"2024-12-11T21:51:34.894868Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-11T21:51:34.896443Z","iopub.execute_input":"2024-12-11T21:51:34.896671Z","iopub.status.idle":"2024-12-11T21:51:36.311616Z","shell.execute_reply.started":"2024-12-11T21:51:34.896648Z","shell.execute_reply":"2024-12-11T21:51:36.310695Z"}},"outputs":[{"name":"stdout","text":"Wed Dec 11 21:51:35 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   63C    P0             29W /   70W |     159MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   37C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# configuration file\nconfig = {\n    \"policy_type\": \"CnnPolicy\",\n    \"total_timesteps\": 1000, # 1000000, 3000000, 20000000\n    \"Algo\": \"PPO\",\n    \"env_name\": \"ALE/DonkeyKong-v5\",\n    \"model_name\": \"ALE/DonkeyKong-v5\",\n    \"Add\": \"reduce_action_add_intermediate_reward_position\",\n    \"export_path\": \"/kaggle/working/exports/\",\n    \"videos_path\": \"/kaggle/working/videos/\",\n}\n\n","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:36.313364Z","iopub.execute_input":"2024-12-11T21:51:36.314043Z","iopub.status.idle":"2024-12-11T21:51:36.504301Z","shell.execute_reply.started":"2024-12-11T21:51:36.314001Z","shell.execute_reply":"2024-12-11T21:51:36.503659Z"},"executionInfo":{"elapsed":291,"status":"ok","timestamp":1733280446854,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"eC5vil4-JnjC","trusted":true},"outputs":[],"execution_count":71},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY \")","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:36.505226Z","iopub.execute_input":"2024-12-11T21:51:36.505481Z","iopub.status.idle":"2024-12-11T21:51:36.866096Z","shell.execute_reply.started":"2024-12-11T21:51:36.505455Z","shell.execute_reply":"2024-12-11T21:51:36.865216Z"},"trusted":true},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# Wandb setup\nwandb.login(key=secret_value_0)\nrun = wandb.init(\n    project=\"Prova\",\n    config=config,\n    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n    save_code=True,  # optional\n)","metadata":{"execution":{"iopub.status.busy":"2024-12-11T21:51:36.867242Z","iopub.execute_input":"2024-12-11T21:51:36.867577Z"},"executionInfo":{"elapsed":3799,"status":"ok","timestamp":1733280455801,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"zkNShWgEjJa1","outputId":"6358f7e5-7ff3-41f7-a5ef-f994f8491c50","trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:wmyef7jq) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.199 MB of 0.199 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5cafe78cb2438c8e5b55cdd31e568b"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"class MyVecTransposeImage(VecEnvWrapper):\n    def __init__(self, venv, skip=False):\n        super().__init__(venv)\n        self.skip = skip\n\n        # Get original shape: e.g., (84, 84, 4)\n        old_shape = self.observation_space.shape\n        # Transpose shape to (C, H, W)\n        new_shape = (old_shape[2], old_shape[0], old_shape[1])  # (4, 84, 84)\n\n        # Use the original low/high if they are uniform; if not, use min/max appropriately\n        low_val = self.observation_space.low.min()\n        high_val = self.observation_space.high.max()\n\n        self.observation_space = gym.spaces.Box(\n            low=low_val,\n            high=high_val,\n            shape=new_shape,\n            dtype=self.observation_space.dtype\n        )\n\n    def reset(self):\n        obs = self.venv.reset()\n        return self.transpose_observations(obs)\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    def step_wait(self):\n        obs, rewards, dones, infos = self.venv.step_wait()\n        return self.transpose_observations(obs), rewards, dones, infos\n\n    def transpose_observations(self, obs):\n        if self.skip:\n            return obs\n        if isinstance(obs, dict):\n            for key, val in obs.items():\n                obs[key] = self._transpose(val)\n            return obs\n        else:\n            return self._transpose(obs)\n\n    def _transpose(self, obs):\n        # obs shape is (n_envs, H, W, C) -> transpose to (n_envs, C, H, W)\n        return obs.transpose(0, 3, 1, 2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_agent_level_position(image):\n    if image is None:\n        raise ValueError(\"Image not loaded. Check the path and file.\")\n    \n    # remove 0 to 25 pixels from the top\n    image = image[32:, :]\n    # plt.imshow(image, cmap='gray')\n\n    image[149:160, 36:44] = 0\n    # display image with black\n    # plt.imshow(image, cmap='gray')\n\n    # Lines detection\n    # copy image\n    gray_image = image.copy()\n\n    # print(\"Image shape:\", gray_image.shape)\n\n    # Perform edge detection\n    edges = cv2.Canny(gray_image, threshold1=30, threshold2=100)\n\n    # Debug: Show edges\n    # plt.figure(figsize=(6, 4))\n    # plt.imshow(edges, cmap='gray')\n    # plt.title(\"Edges\")\n    # plt.axis(\"off\")\n    # plt.show()\n\n    # Detect horizontal lines using Hough Transform\n    lines = cv2.HoughLinesP(\n        edges, \n        rho=1, \n        theta=np.pi / 180, \n        threshold=30, \n        minLineLength=10, \n        maxLineGap=20\n    )\n\n    # Draw detected lines on a debug image\n    debug_line_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    horizontal_lines = []\n\n    if lines is not None:\n        # print(f\"Total lines detected (before filtering): {len(lines)}\")\n        for line in lines:\n            x1, y1, x2, y2 = line[0]\n            # Check for horizontal lines with a more lenient threshold\n            vertical_diff = abs(y2 - y1)\n            horizontal_diff = abs(x2 - x1)\n            \n            if vertical_diff < horizontal_diff * 0.1:  # Allow slight vertical tilt\n                horizontal_lines.append((x1, y1, x2, y2))\n                # cv2.line(debug_line_image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue for horizontal lines\n    # else:\n    #     print(\"No lines detected.\")\n\n    # Debug: Show all detected horizontal lines\n    # plt.figure(figsize=(10, 6))\n    # plt.imshow(cv2.cvtColor(debug_line_image, cv2.COLOR_BGR2RGB))\n    # plt.title(\"Detected Horizontal Lines (Blue)\")\n    # plt.axis(\"off\")\n    # plt.show()\n\n    # Print detected horizontal lines for debugging\n    # print(f\"Total horizontal lines detected (after filtering): {len(horizontal_lines)}\")\n    # for idx, (x1, y1, x2, y2) in enumerate(horizontal_lines):\n    #     print(f\"Line {idx + 1}: ({x1}, {y1}) to ({x2}, {y2})\")\n\n    # detect the agent and it position\n    # Perform binary thresholding to highlight the agent and objects\n    _, binary = cv2.threshold(gray_image, 50, 255, cv2.THRESH_BINARY)\n\n    # Apply morphological operations to clean up noise\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n    binary_cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n\n    # Debug: Show the binary image after cleaning\n    # plt.figure(figsize=(6, 4))\n    # plt.imshow(binary_cleaned, cmap='gray')\n    # plt.title(\"Binary Image (Cleaned)\")\n    # plt.axis(\"off\")\n    # plt.show()\n\n    # Detect contours in the cleaned binary image\n    contours, _ = cv2.findContours(binary_cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Initialize variables for the agent's position\n    agent_detection_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    agent_position = None\n\n    # Filter contours to find the agent\n    # print(f\"Total contours detected: {len(contours)}\")\n\n    for contour in contours:\n        # Get bounding box of the contour\n        x, y, w, h = cv2.boundingRect(contour)\n        \n        # Filter based on size: Assuming agent is small\n        if 5 <= w <= 30 and 5 <= h <= 30:  # Adjust range based on resolution\n            # Further filter based on aspect ratio to avoid line-like objects\n            aspect_ratio = max(w / h, h / w)\n            if aspect_ratio < 2.0:  # Allow only nearly square contours\n                agent_position = (x + w // 2, y + h // 2)  # Center of the bounding box\n                \n                # Draw the bounding box and center point for the agent\n                # cv2.rectangle(agent_detection_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n                # cv2.circle(agent_detection_image, agent_position, radius=5, color=(0, 0, 255), thickness=-1)\n                # print(f\"Agent detected at: {agent_position}, size: {w}x{h}\")\n                break  # Assuming only one agent in the frame\n\n    # if agent_position is None:\n    #     print(\"No agent detected. Try adjusting size and aspect ratio filters.\")\n\n    # Display the agent detection result\n    # plt.figure(figsize=(10, 6))\n    # plt.imshow(cv2.cvtColor(agent_detection_image, cv2.COLOR_BGR2RGB))\n    # plt.title(\"Agent Detection\")\n    # plt.axis(\"off\")\n    # plt.show()\n\n    \n    # Detect level\n    # Sort lines by their average y-value, descending (bottom to top)\n    lines_sorted = sorted(horizontal_lines, key=lambda line: (line[1] + line[3]) / 2, reverse=True)\n\n    def cluster_lines(lines, desired_clusters=7, proximity_threshold=10):\n        clusters = []\n        current_cluster = [lines[0]]\n        for line in lines[1:]:\n            line_y = (line[1] + line[3]) // 2\n            current_cluster_y = sum((l[1]+l[3])//2 for l in current_cluster) / len(current_cluster)\n            # If the difference is small, add to current cluster, else start a new one\n            if abs(line_y - current_cluster_y) < proximity_threshold:\n                current_cluster.append(line)\n            else:\n                clusters.append(current_cluster)\n                current_cluster = [line]\n        clusters.append(current_cluster)\n        \n        # If we get more or fewer than desired_clusters, adjust proximity_threshold and try again\n        # For simplicity, if we don't get exactly 7, we might raise an error or adjust threshold.\n        # But here we assume we pick a threshold that works.\n        # if len(clusters) != desired_clusters:\n        #     # Adjust proximity_threshold manually or implement a loop to find a suitable threshold.\n        #     print(f\"Expected {desired_clusters} clusters, got {len(clusters)}. Adjust proximity_threshold.\")\n        return clusters\n\n    proximity_threshold = 10  # Adjust as needed\n    clusters = cluster_lines(lines_sorted, desired_clusters=7, proximity_threshold=proximity_threshold)\n\n    # Compute representative y-value for each cluster (average)\n    boundary_y_values = []\n    for cluster in clusters:\n        avg_y = sum((l[1] + l[3]) // 2 for l in cluster) / len(cluster)\n        boundary_y_values.append(avg_y)\n\n    # Sort boundaries again in descending order (bottom = largest y, top = smallest y)\n    boundary_y_values.sort(reverse=True)\n\n    # Now we have 7 boundaries for 6 levels:\n    # Level 1: between boundary_y_values[0] and boundary_y_values[1]\n    # Level 2: between boundary_y_values[1] and boundary_y_values[2]\n    # ...\n    # Level 6: between boundary_y_values[5] and boundary_y_values[6]\n\n    agent_level = None\n    if agent_position:\n        agent_y = agent_position[1]\n        # Find which level agent_y falls into\n        for i in range(6):\n            if boundary_y_values[i] >= agent_y > boundary_y_values[i+1]:\n                agent_level = i + 1\n                break\n\n    # Print only the agent's level\n    # if agent_level is not None:\n    #     print(agent_level)\n    # else:\n    #     print(\"Could not determine agent's level.\")\n    # return_not_detected_agent = 0\n    # if agent_level is None:\n    #     # print(\"Could not determine agent's level.\")\n    #     return_not_detected_agent = 1\n\n    # Draw minimal annotation: just draw the agent and print its level\n    final_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    # if agent_position:\n    #     cv2.circle(final_image, agent_position, radius=5, color=(0, 0, 255), thickness=-1)\n    #     if agent_level is not None:\n    #         cv2.putText(\n    #             final_image, \n    #             f\"Level: {agent_level}\", \n    #             (agent_position[0] + 10, agent_position[1] - 10), \n    #             cv2.FONT_HERSHEY_SIMPLEX, \n    #             0.5, \n    #             (0, 255, 0), \n    #             1\n    #         )\n\n    # plt.figure(figsize=(10, 6))\n    # plt.imshow(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))\n    # plt.axis(\"off\")\n    # plt.show()\n\n    return agent_level, agent_position# agent position is (x, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class IntermediateRewardWrapper(gym.Wrapper):\n    def __init__(self, env):\n        super(IntermediateRewardWrapper, self).__init__(env)\n        self.ladder_postion = [110,82,90,70,110,78]\n        self.last_level = 1\n        self.previous_additional_reward = 0.0\n        self.last_y_position = 0\n\n    def step(self, action):\n\n        obs, reward, terminated, truncated, info = self.env.step(action)\n        # print(f\"from intermediate reward {obs.shape}\")\n        # print(f\"from intermediate reward {reward}\")\n\n        # done = terminated or truncated\n\n        additional_reward = 0.0\n        # get agent level and position\n        agent_level, agent_position = get_agent_level_position(obs)\n        \n\n\n        if agent_level is not None and 1 <= agent_level <= len(self.ladder_postion):\n            agent_level_reward = (7 - agent_level)* (-0.01)\n            additional_reward += agent_level_reward\n            \n            if agent_level > self.last_level:\n                additional_reward += 500.0\n                self.last_level = agent_level\n\n            diff = 0\n            if agent_position is not None and len(agent_position) > 0:\n                # get absolute difference between agent position and ladder position\n                diff = abs(self.ladder_postion[agent_level - 1] - agent_position[0])\n                agent_position_reward = diff * (-0.1)\n                additional_reward += agent_position_reward \n            \n                if action == 2 and diff <= 1 and agent_position[1] > self.last_y_position:\n                    additional_reward += 25.0\n                self.last_y_position = agent_position[1]\n            \n\n\n        else:\n            # If agent_level or agent_position not found, use the previous reward\n            additional_reward = self.previous_additional_reward\n\n\n        # Round the additional reward to 2 decimal places\n        additional_reward = round(additional_reward, 2)\n\n        # Update the previous reward\n        self.previous_additional_reward = additional_reward\n\n        # Add the additional reward to the original reward\n        reward += additional_reward\n\n        # print(f\"obs: {obs.shape} , Agent Level: {agent_level}, Agent Position: {agent_position}, Additional Reward: {additional_reward}, Total Reward: {reward}\")\n        \n        return obs, reward, terminated, truncated, info\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ActionFilterWrapper(gym.ActionWrapper):\n    def __init__(self, env, allowed_actions):\n        super().__init__(env)\n        self.allowed_actions = allowed_actions\n        # The new action space matches the number of allowed actions\n        self.action_space = Discrete(len(self.allowed_actions))\n\n    def action(self, act):\n        # Map the reduced action space index to the original action\n        return self.allowed_actions[act]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        # The original shape remains (84,84,1), but the dtype and range change\n        self.observation_space = gym.spaces.Box(\n            low=0.0,\n            high=1.0,\n            shape=self.observation_space.shape,\n            dtype=np.float32\n        )\n        \n    def observation(self, obs):\n        return np.array(obs).astype(np.float32) / 255.0\n\n\n# class ScaledFloatFrame(gym.ObservationWrapper):\n#     def observation(self, obs):\n#         return np.array(obs).astype(np.float32) / 255.0\n\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env=None):\n        super().__init__(env)\n        # Check that 'FIRE' is a valid action in the environment\n        assert 'FIRE' in env.unwrapped.get_action_meanings(), \"Environment does not support 'FIRE' action\"\n        assert len(env.unwrapped.get_action_meanings()) >= 3, \"Action space too small for expected actions\"\n\n    def step(self, action):\n        return self.env.step(action)\n\n    def reset(self, **kwargs):\n        # Reset the environment\n        obs, info = self.env.reset(**kwargs)\n\n        # Perform the FIRE action\n        obs, _, terminated, truncated, _ = self.env.step(1)\n        if terminated or truncated:  # If game ends after FIRE, reset again\n            obs, info = self.env.reset(**kwargs)\n\n        return obs, info\n        \n# Custom wrapper to add channel dimension\nclass AddChannelDimension(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape\n        # Update the observation space to include a channel dimension\n        self.observation_space = gym.spaces.Box(\n            low=0,\n            high=255,\n            shape=(obs_shape[0], obs_shape[1], 1),\n            dtype=np.uint8,\n        )\n\n    def observation(self, observation):\n        # Add a channel dimension\n        return np.expand_dims(observation, axis=-1)\n\n# class MaxAndSkipEnv(gym.Wrapper):\n#     def __init__(self, env=None, skip=4):\n#         super(MaxAndSkipEnv, self).__init__(env)\n#         self._obs_buffer = collections.deque(maxlen=2)\n#         self._skip = skip\n\n#     def step(self, action):\n#         total_reward = 0.0\n#         done = None\n#         for _ in range(self._skip):\n#             obs, reward, terminated,truncated, info = self.env.step(action)\n#             done = terminated or truncated\n#             self._obs_buffer.append(obs)\n#             total_reward += reward\n#             if done:\n#                 break\n#         max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n#         return max_frame, total_reward, terminated, truncated, info\n\n    # def reset(self, *, seed=None, options=None):\n    #     self._obs_buffer.clear()\n    #     obs, info = self.env.reset(seed=seed, options=options)\n    #     self._obs_buffer.append(obs)\n    #     return obs, info\n\n\ndef make_env(env_name, allowed_actions, obs_type=\"grayscale\", render_mode=None,):\n    def _init():\n        env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n        print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n        env = FireResetEnv(env)\n        print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n        # Wrap the environment with the custom ActionFilterWrapper\n        env = ActionFilterWrapper(env, allowed_actions)\n        print(\"ActionFilterWrapper   : {}\".format(env.observation_space.shape))\n        # Wrap the environment to add intermediate rewards\n        env = IntermediateRewardWrapper(env)\n        print(\"IntermediateReward    : {}\".format(env.observation_space.shape))\n        # env = MaxAndSkipEnv(env)\n        # print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n        env = ResizeObservation(env, (84, 84))\n        print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n        env = AddChannelDimension(env)  # Add channel dimension here\n        print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n        \n        env = ScaledFloatFrame(env)\n        print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n        \n        # env = Monitor(env, allow_early_resets=False) # from stable baselines\n        # print(\"Monitor               : {}\".format(env.observation_space.shape))\n\n        return env\n    return _init\n\n\n# select relevant actions\nallowed_actions = [0, 1, 2, 3, 4, 5, 11, 12]\n# env = DummyVecEnv([make_env(config[\"env_name\"])])\n# use make_vec_env with n_envs=16\nenv = make_vec_env(env_id=make_env(config[\"env_name\"], allowed_actions= allowed_actions), n_envs=8)\n\n# stack 4 frames\nenv = VecFrameStack(env, n_stack=4)\nprint(\"Post VecFrameStack Shape: {}\".format(env.observation_space.shape))\n\n# convert back to PyTorch format (channel-first)\n# env = VecTransposeImage(env)\nenv = MyVecTransposeImage(env)\nprint(\"VecTransposeImage Shape: {}\".format(env.observation_space.shape))\n\nprint(\"Final Observation Space: {}\".format(env.observation_space.shape))","metadata":{"executionInfo":{"elapsed":1160,"status":"ok","timestamp":1733280459863,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"sPknvIhmJnjC","outputId":"854f052d-d675-4c90-eca4-fac5440888aa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Start Testing","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class ScaledFloatFrame(gym.ObservationWrapper):\n#     def __init__(self, env):\n#         super().__init__(env)\n#         # The original shape remains (84,84,1), but the dtype and range change\n#         self.observation_space = gym.spaces.Box(\n#             low=0.0,\n#             high=1.0,\n#             shape=self.observation_space.shape,\n#             dtype=np.float32\n#         )\n        \n#     def observation(self, obs):\n#         return np.array(obs).astype(np.float32) / 255.0\n\n\n# # class ScaledFloatFrame(gym.ObservationWrapper):\n# #     def observation(self, obs):\n# #         return np.array(obs).astype(np.float32) / 255.0\n\n\n\n# class FireResetEnv(gym.Wrapper):\n#     def __init__(self, env=None):\n#         super().__init__(env)\n#         # Check that 'FIRE' is a valid action in the environment\n#         assert 'FIRE' in env.unwrapped.get_action_meanings(), \"Environment does not support 'FIRE' action\"\n#         assert len(env.unwrapped.get_action_meanings()) >= 3, \"Action space too small for expected actions\"\n\n#     def step(self, action):\n#         return self.env.step(action)\n\n#     def reset(self, **kwargs):\n#         # Reset the environment\n#         obs, info = self.env.reset(**kwargs)\n\n#         # Perform the FIRE action\n#         obs, _, terminated, truncated, _ = self.env.step(1)\n#         if terminated or truncated:  # If game ends after FIRE, reset again\n#             obs, info = self.env.reset(**kwargs)\n\n#         return obs, info\n        \n# # Custom wrapper to add channel dimension\n# class AddChannelDimension(gym.ObservationWrapper):\n#     def __init__(self, env):\n#         super().__init__(env)\n#         obs_shape = self.observation_space.shape\n#         # Update the observation space to include a channel dimension\n#         self.observation_space = gym.spaces.Box(\n#             low=0,\n#             high=255,\n#             shape=(obs_shape[0], obs_shape[1], 1),\n#             dtype=np.uint8,\n#         )\n\n#     def observation(self, observation):\n#         # Add a channel dimension\n#         return np.expand_dims(observation, axis=-1)\n\n# # class MaxAndSkipEnv(gym.Wrapper):\n# #     def __init__(self, env=None, skip=4):\n# #         super(MaxAndSkipEnv, self).__init__(env)\n# #         self._obs_buffer = collections.deque(maxlen=2)\n# #         self._skip = skip\n\n# #     def step(self, action):\n# #         total_reward = 0.0\n# #         done = None\n# #         for _ in range(self._skip):\n# #             obs, reward, terminated,truncated, info = self.env.step(action)\n# #             done = terminated or truncated\n# #             self._obs_buffer.append(obs)\n# #             total_reward += reward\n# #             if done:\n# #                 break\n# #         max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n# #         return max_frame, total_reward, terminated, truncated, info\n\n#     # def reset(self, *, seed=None, options=None):\n#     #     self._obs_buffer.clear()\n#     #     obs, info = self.env.reset(seed=seed, options=options)\n#     #     self._obs_buffer.append(obs)\n#     #     return obs, info\n\n\n# def make_env(env_name, obs_type=\"grayscale\", render_mode=None,):\n#     def _init():\n#         env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n#         print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n#         env = FireResetEnv(env)\n#         print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n#         # env = MaxAndSkipEnv(env)\n#         # print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n#         env = ResizeObservation(env, (84, 84))\n#         print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n#         env = AddChannelDimension(env)  # Add channel dimension here\n#         print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n#         # env = GrayscaleObservation(env, keep_dim=True)\n#         # print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n        \n#         env = ScaledFloatFrame(env)\n#         print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n        \n#         # env = Monitor(env, allow_early_resets=False) # from stable baselines\n#         # print(\"Monitor               : {}\".format(env.observation_space.shape))\n\n#         return env\n#     return _init\n\n\n\n# # env = DummyVecEnv([make_env(config[\"env_name\"])])\n# # use make_vec_env with n_envs=16\n# env = make_vec_env(env_id=make_env(config[\"env_name\"]), n_envs=1) # ALE/MarioBros-v5\n\n# # stack 4 frames\n# env = VecFrameStack(env, n_stack=4)\n# print(\"Post VecFrameStack Shape: {}\".format(env.observation_space.shape))\n\n# # convert back to PyTorch format (channel-first)\n# # env = VecTransposeImage(env)\n# env = MyVecTransposeImage(env)\n# print(\"VecTransposeImage Shape: {}\".format(env.observation_space.shape))\n\n# print(\"Final Observation Space: {}\".format(env.observation_space.shape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# obs = env.reset()\n# print(\"Initial observation shape:\", obs.shape)\n# done = False\n# total_reward = 0.0\n\n# for i in range(10000):\n#     # Take a random action\n#     action = env.action_space.sample()\n#     obs, reward, done, info = env.step([action])\n#     total_reward += reward\n\n#     # if i % 50 == 0:\n#     #     print(f\"Step: {i}, Reward: {reward}, Done: {done}, Obs mean: {obs.mean()}\")\n#     if reward>0:\n#         print(f\"Step: {i}, Reward: {reward}, Done: {done}, Obs mean: {obs.mean()}\")\n\n#     if done:\n#         print(\"Done\")\n#         obs = env.reset()\n#         total_reward = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class ScaledFloatFrame(gym.ObservationWrapper):\n#     def __init__(self, env):\n#         super().__init__(env)\n#         # The original shape remains (84,84,1), but the dtype and range change\n#         self.observation_space = gym.spaces.Box(\n#             low=0.0,\n#             high=1.0,\n#             shape=self.observation_space.shape,\n#             dtype=np.float32\n#         )\n        \n#     def observation(self, obs):\n#         return np.array(obs).astype(np.float32) / 255.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def make_env(env_name, obs_type=\"grayscale\", render_mode=None,):\n#     def _init():\n#         env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n#         print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n#         env = FireResetEnv(env)\n#         print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n#         env = ResizeObservation(env, (84, 84))\n#         print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n#         env = ResizeObservation(env, (84, 84))\n#         print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n#         env = AddChannelDimension(env)  # Add channel dimension here\n#         print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n#         env = ScaledFloatFrame(env)\n#         print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n\n#         return env\n#     return _init","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # env = gym.make(\"ALE/Pong-v5\", obs_type=\"grayscale\")\n# # print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n# # env = FireResetEnv(env)\n# # print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n# # env = ResizeObservation(env, (84, 84))\n# # print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n# # env = AddChannelDimension(env)\n# # print(\"AddChannelDimension   : {}\".format(env.observation_space.shape))\n# # env = ScaledFloatFrame(env)\n# # print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n# # env = Monitor(env, allow_early_resets=False) # from stable baselines\n# # print(\"Monitor               : {}\".format(env.observation_space.shape))\n# env = make_vec_env(env_id=make_env(config[\"env_name\"]), n_envs=1)\n# obs = env.reset()\n# print(obs.shape)\n# print(obs)\n# print(\"Initial Obs Stats -> min:\", obs.min(), \"max:\", obs.max(), \"mean:\", obs.mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# End Testing","metadata":{}},{"cell_type":"code","source":"print(\"Check\")\nprint(\"Post VecFrameStack Shape: {}\".format(env.observation_space.shape))\nprint(\"Final Observation Space: {}\".format(env.observation_space.shape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def make_env_evual(env_name, obs_type=\"grayscale\", render_mode=None,):\n#     def _init():\n#         env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n#         print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n#         env = FireResetEnv(env)\n#         print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n#         # env = MaxAndSkipEnv(env)\n#         # print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n#         env = ResizeObservation(env, (84, 84))\n#         print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n#         env = AddChannelDimension(env)  # Add channel dimension here\n#         print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n#         # env = GrayscaleObservation(env, keep_dim=True)\n#         # print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n#         env = ScaledFloatFrame(env)\n#         print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n#         # env = Monitor(env, allow_early_resets=False) # from stable baselines\n#         # print(\"Monitor               : {}\".format(env.observation_space.shape))\n\n#         return env\n#     return _init\n\n\n# Create an evaluation environment (similar to our training env)\neval_env = make_vec_env(env_id=make_env(config[\"env_name\"], allowed_actions= allowed_actions), n_envs=1)\n\n# stack 4 frames\neval_env = VecFrameStack(eval_env, n_stack=4)\nprint(\"eval_env Post VecFrameStack Shape: {}\".format(eval_env.observation_space.shape))\n\n# convert back to PyTorch format (channel-first)\n# eval_env = VecTransposeImage(eval_env)\neval_env = MyVecTransposeImage(eval_env)\nprint(\"eval_env MyVecTransposeImage Shape: {}\".format(eval_env.observation_space.shape))\nprint(\"eval_env Final Observation Space: {}\".format(eval_env.observation_space.shape))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# obs = eval_env.reset()\n# print(obs.shape)\n# print(obs)\n# print(\"Initial Obs Stats -> min:\", obs.min(), \"max:\", obs.max(), \"mean:\", obs.mean())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define\n# batch_size=30048,\n# policy_kwargs = dict(net_arch=[512, 256])\n# policy_kwargs=dict(net_arch=[512, 256], normalize_images=False)\nmodel = PPO(config[\"policy_type\"], \n            env, \n            verbose=0, \n            tensorboard_log=f\"runs/{run.id}\",\n            batch_size=256, \n            learning_rate=2.5e-4, #2.5e-4, 0.001\n            gamma=0.99,\n            n_steps=128,\n            n_epochs=4,\n            clip_range=0.1,\n            vf_coef=0.5,\n            ent_coef=0.05, # 0.01\n            policy_kwargs=dict(net_arch=[512, 256], normalize_images=False),\n            device=\"cuda\")\n","metadata":{"executionInfo":{"elapsed":4533219,"status":"ok","timestamp":1733284994133,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"6YPvHbZ1f1aS","outputId":"ae848be6-36fe-4984-f2aa-d1c6f628453a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the evaluation callback\neval_callback = EvalCallback(\n    eval_env,\n    best_model_save_path=config[\"export_path\"],  # directory to save the best model\n    log_path=config[\"export_path\"],              # evaluation logs\n    eval_freq=500,                            # evaluate the model every 500,000 steps\n    deterministic=True,\n    render=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GradientInspectionCallback(BaseCallback):\n    def __init__(self, verbose=0):\n        super(GradientInspectionCallback, self).__init__(verbose)\n    \n    def _on_step(self) -> bool:\n        # Access the policy network\n        policy_net = self.model.policy\n        \n        # Iterate over the parameters to inspect gradients\n        for name, param in policy_net.named_parameters():\n            if param.grad is not None:\n                grad_norm = param.grad.norm().item()\n                self.logger.record(f\"gradients/{name}_norm\", grad_norm)\n        \n        return True  # Continue training\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DebugObservationCallback(BaseCallback):\n    def __init__(self, verbose=0):\n        super(DebugObservationCallback, self).__init__(verbose)\n\n    def _on_step(self) -> bool:\n        # Get the current observation\n        observation = self.locals[\"new_obs\"]  # Observations from the environment\n        print(\"Observation mean:\", np.mean(observation))\n        print(\"Observation std:\", np.std(observation))\n        print(\"Observation min:\", np.min(observation))\n        print(\"Observation max:\", np.max(observation))\n\n        # Stop training if all observations are zero (optional)\n        # if np.all(observation == 0):\n        #     print(\"All observation values are zero. Exiting...\")\n        #     return False\n\n        return True  # Continue training","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize our custom callback\n# wandb_best_model_callback = WandbBestModelCallback(\n#     best_model_path=config[\"export_path\"],\n#     artifact_name=\"best_model\",\n#     project_name=\"donkey_kong_additional_reward\",  # replace with your project name\n#     entity=None,  # add entity if you have one\n#     verbose=1\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine both callbacks\n# callbacks = CallbackList([eval_callback, WandbCallback(verbose=2), GradientInspectionCallback(), DebugObservationCallback()])\ncallbacks = CallbackList([eval_callback, WandbCallback(verbose=2), GradientInspectionCallback()])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train\nt0 = datetime.now()\nmodel.learn(total_timesteps=config[\"total_timesteps\"], callback=callbacks)\nt1 = datetime.now()\nprint('>>> Training time (hh:mm:ss.ms): {}'.format(t1-t0))\n\n# save and export model\nmodel.save(config[\"export_path\"] + config[\"model_name\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training time without parallezing env: >>> Training time (hh:mm:ss.ms): 1:01:58.577332","metadata":{}},{"cell_type":"code","source":"# finish wandb project\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Finish\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n# shutil.rmtree(\"/kaggle/working/exports\")\n# os.remove(\"/kaggle/working/Breakout-v5.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip -r folder_name.zip exports\n# from google.colab import files\n# files.download('folder_name.zip')","metadata":{"executionInfo":{"elapsed":1702,"status":"ok","timestamp":1733284995819,"user":{"displayName":"Amrit cheema","userId":"09351746591115935562"},"user_tz":-60},"id":"so9okg9mrpk2","outputId":"2413d966-b253-403c-a068-66de36159e6d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n#         print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n#         env = FireResetEnv(env)\n#         print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n#         # env = MaxAndSkipEnv(env)\n#         # print(\"MaxAndSkipEnv        : {}\".format(env.observation_space.shape))\n#         env = ResizeObservation(env, (84, 84))\n#         print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n#         env = AddChannelDimension(env)  # Add channel dimension here\n#         print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n#         # env = GrayscaleObservation(env, keep_dim=True)\n#         # print(\"GrayscaleObservation : {}\".format(env.observation_space.shape))\n#         env = ScaledFloatFrame(env)\n#         print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n#         env = Monitor(env, allow_early_resets=False) # from stable baselines\n#         print(\"Monitor ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}