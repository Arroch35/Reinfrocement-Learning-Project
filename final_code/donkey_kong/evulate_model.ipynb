{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ale_py\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "from gymnasium.wrappers import MaxAndSkipObservation, ResizeObservation, GrayscaleObservation, FrameStackObservation, ReshapeObservation\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "gym.register_envs(ale_py)\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.ppo.policies import MlpPolicy\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import collections\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecEnvWrapper\n",
    "from stable_baselines3 import PPO\n",
    "import cv2\n",
    "from gymnasium.spaces import Discrete\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"best_model.zip\"\n",
    "num_episodes = 2 # number of episodes to run\n",
    "climb_auto = True # if True, the agent will automatically climb ladders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration file\n",
    "config = {\n",
    "    \"policy_type\": \"CnnPolicy\",\n",
    "    \"total_timesteps\": 1000000,\n",
    "    \"env_name\": \"ALE/DonkeyKong-v5\", \n",
    "    \"model_name\": \"ALE/DonkeyKong-v5\",\n",
    "    \"export_path\": \"./exports/\",\n",
    "    \"videos_path\": \"./videos/\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVecTransposeImage(VecEnvWrapper):\n",
    "    def __init__(self, venv, skip=False):\n",
    "        super().__init__(venv)\n",
    "        self.skip = skip\n",
    "\n",
    "        # Get original shape: e.g., (84, 84, 4)\n",
    "        old_shape = self.observation_space.shape\n",
    "        # Transpose shape to (C, H, W)\n",
    "        new_shape = (old_shape[2], old_shape[0], old_shape[1])  # (4, 84, 84)\n",
    "\n",
    "        # Use the original low/high if they are uniform; if not, use min/max appropriately\n",
    "        low_val = self.observation_space.low.min()\n",
    "        high_val = self.observation_space.high.max()\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=low_val,\n",
    "            high=high_val,\n",
    "            shape=new_shape,\n",
    "            dtype=self.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.venv.reset()\n",
    "        return self.transpose_observations(obs)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        self.venv.step_async(actions)\n",
    "\n",
    "    def step_wait(self):\n",
    "        obs, rewards, dones, infos = self.venv.step_wait()\n",
    "        return self.transpose_observations(obs), rewards, dones, infos\n",
    "\n",
    "    def transpose_observations(self, obs):\n",
    "        if self.skip:\n",
    "            return obs\n",
    "        if isinstance(obs, dict):\n",
    "            for key, val in obs.items():\n",
    "                obs[key] = self._transpose(val)\n",
    "            return obs\n",
    "        else:\n",
    "            return self._transpose(obs)\n",
    "\n",
    "    def _transpose(self, obs):\n",
    "        # obs shape is (n_envs, H, W, C) -> transpose to (n_envs, C, H, W)\n",
    "        return obs.transpose(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_level_position(image):\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not loaded. Check the path and file.\")\n",
    "    \n",
    "    # remove 0 to 25 pixels from the top\n",
    "    image = image[32:, :]\n",
    "    # plt.imshow(image, cmap='gray')\n",
    "\n",
    "    image[149:160, 36:44] = 0\n",
    "    # display image with black\n",
    "    # plt.imshow(image, cmap='gray')\n",
    "\n",
    "    # Lines detection\n",
    "    # copy image\n",
    "    gray_image = image.copy()\n",
    "\n",
    "    # print(\"Image shape:\", gray_image.shape)\n",
    "\n",
    "    # Perform edge detection\n",
    "    edges = cv2.Canny(gray_image, threshold1=30, threshold2=100)\n",
    "\n",
    "\n",
    "    # Detect horizontal lines using Hough Transform\n",
    "    lines = cv2.HoughLinesP(\n",
    "        edges, \n",
    "        rho=1, \n",
    "        theta=np.pi / 180, \n",
    "        threshold=30, \n",
    "        minLineLength=10, \n",
    "        maxLineGap=20\n",
    "    )\n",
    "\n",
    "    # Draw detected lines on a debug image\n",
    "    debug_line_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    horizontal_lines = []\n",
    "\n",
    "    if lines is not None:\n",
    "        # print(f\"Total lines detected (before filtering): {len(lines)}\")\n",
    "        for line in lines:\n",
    "            x1, y1, x2, y2 = line[0]\n",
    "            # Check for horizontal lines with a more lenient threshold\n",
    "            vertical_diff = abs(y2 - y1)\n",
    "            horizontal_diff = abs(x2 - x1)\n",
    "            \n",
    "            if vertical_diff < horizontal_diff * 0.1:  # Allow slight vertical tilt\n",
    "                horizontal_lines.append((x1, y1, x2, y2))\n",
    "                # cv2.line(debug_line_image, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue for horizontal lines\n",
    "    # else:\n",
    "    #     print(\"No lines detected.\")\n",
    "\n",
    "\n",
    "    # detect the agent and it position\n",
    "    # Perform binary thresholding to highlight the agent and objects\n",
    "    _, binary = cv2.threshold(gray_image, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Apply morphological operations to clean up noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    binary_cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Detect contours in the cleaned binary image\n",
    "    contours, _ = cv2.findContours(binary_cleaned, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize variables for the agent's position\n",
    "    agent_detection_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "    agent_position = None\n",
    "\n",
    "    # Filter contours to find the agent\n",
    "    # print(f\"Total contours detected: {len(contours)}\")\n",
    "\n",
    "    for contour in contours:\n",
    "        # Get bounding box of the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Filter based on size: Assuming agent is small\n",
    "        if 5 <= w <= 30 and 5 <= h <= 30:  # Adjust range based on resolution\n",
    "            # Further filter based on aspect ratio to avoid line-like objects\n",
    "            aspect_ratio = max(w / h, h / w)\n",
    "            if aspect_ratio < 2.0:  # Allow only nearly square contours\n",
    "                agent_position = (x + w // 2, y + h // 2)  # Center of the bounding box\n",
    "                \n",
    "\n",
    "                break  # Assuming only one agent in the frame\n",
    "\n",
    "    \n",
    "    # Detect level\n",
    "    # Sort lines by their average y-value, descending (bottom to top)\n",
    "    lines_sorted = sorted(horizontal_lines, key=lambda line: (line[1] + line[3]) / 2, reverse=True)\n",
    "\n",
    "    def cluster_lines(lines, desired_clusters=7, proximity_threshold=10):\n",
    "        clusters = []\n",
    "        current_cluster = [lines[0]]\n",
    "        for line in lines[1:]:\n",
    "            line_y = (line[1] + line[3]) // 2\n",
    "            current_cluster_y = sum((l[1]+l[3])//2 for l in current_cluster) / len(current_cluster)\n",
    "            # If the difference is small, add to current cluster, else start a new one\n",
    "            if abs(line_y - current_cluster_y) < proximity_threshold:\n",
    "                current_cluster.append(line)\n",
    "            else:\n",
    "                clusters.append(current_cluster)\n",
    "                current_cluster = [line]\n",
    "        clusters.append(current_cluster)\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "    proximity_threshold = 10  # Adjust as needed\n",
    "    clusters = cluster_lines(lines_sorted, desired_clusters=7, proximity_threshold=proximity_threshold)\n",
    "\n",
    "    # Compute representative y-value for each cluster (average)\n",
    "    boundary_y_values = []\n",
    "    for cluster in clusters:\n",
    "        avg_y = sum((l[1] + l[3]) // 2 for l in cluster) / len(cluster)\n",
    "        boundary_y_values.append(avg_y)\n",
    "\n",
    "    # Sort boundaries again in descending order (bottom = largest y, top = smallest y)\n",
    "    boundary_y_values.sort(reverse=True)\n",
    "\n",
    "    agent_level = None\n",
    "    if agent_position:\n",
    "        agent_y = agent_position[1]\n",
    "        # Find which level agent_y falls into\n",
    "        for i in range(6):\n",
    "            if boundary_y_values[i] >= agent_y > boundary_y_values[i+1]:\n",
    "                agent_level = i + 1\n",
    "                break\n",
    "\n",
    "\n",
    "    # Draw minimal annotation: just draw the agent and print its level\n",
    "    final_image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "    return agent_level, agent_position# agent position is (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For intermediate rewards\n",
    "class IntermediateRewardWrapper_climb_not_auto(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(IntermediateRewardWrapper_climb_not_auto, self).__init__(env)\n",
    "        self.ladder_postion = [110,82,90,70,110,78]\n",
    "        self.last_level = 1\n",
    "        self.previous_additional_reward = 0.0\n",
    "        self.last_y_position = 0\n",
    "        # self.num_preprocessings = 1\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        # print(f\"from intermediate reward {obs.shape}\")\n",
    "        # print(f\"from intermediate reward {reward}\")\n",
    "\n",
    "        # done = terminated or truncated\n",
    "\n",
    "        # reward = reward * 1.5\n",
    "        additional_reward = 0.0\n",
    "\n",
    "        # if self.num_preprocessings == 1:\n",
    "        # get agent level and position\n",
    "        agent_level, agent_position = get_agent_level_position(obs)\n",
    "\n",
    " \n",
    "\n",
    "        if agent_level is not None and 1 <= agent_level <= len(self.ladder_postion):\n",
    "            agent_level_reward = (7 - agent_level)* (-0.01)\n",
    "            additional_reward += agent_level_reward\n",
    "\n",
    "            if agent_level > self.last_level:\n",
    "                additional_reward += 5000.0\n",
    "                self.last_level = agent_level\n",
    "            \n",
    "\n",
    "            diff = 0\n",
    "            if agent_position is not None and len(agent_position) > 0:\n",
    "                # get absolute difference between agent position and ladder position\n",
    "                diff = abs(self.ladder_postion[agent_level - 1] - agent_position[0])\n",
    "                agent_position_reward = diff * (-0.1)\n",
    "                additional_reward += agent_position_reward\n",
    "\n",
    "                if action == 2 and diff <= 1 and agent_position[1] > self.last_y_position:\n",
    "                    additional_reward += 30.0\n",
    "                self.last_y_position = agent_position[1]\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            # If agent_level or agent_position not found, use the previous reward\n",
    "            additional_reward = self.previous_additional_reward\n",
    "\n",
    "\n",
    "        # Round the additional reward to 2 decimal places\n",
    "        additional_reward = round(additional_reward, 2)\n",
    "\n",
    "        # Update the previous reward\n",
    "        self.previous_additional_reward = additional_reward\n",
    "\n",
    "        #     self.num_preprocessings = 0\n",
    "        # else:\n",
    "        #     self.num_preprocessings = 1\n",
    "        #     additional_reward = self.previous_additional_reward\n",
    "\n",
    "\n",
    "        # Add the additional reward to the original reward\n",
    "        reward += additional_reward\n",
    "\n",
    "        # print(f\"obs: {obs.shape} , Agent Level: {agent_level}, Agent Position: {agent_position}, Additional Reward: {additional_reward}, Total Reward: {reward}\")\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateRewardWrapper_climb_auto(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(IntermediateRewardWrapper_climb_auto, self).__init__(env)\n",
    "        self.ladder_postion = [110,82,90,70,110,78]\n",
    "        self.last_level = 1\n",
    "        self.previous_additional_reward = 0.0\n",
    "        self.last_y_position = 0\n",
    "        # self.num_preprocessings = 1\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        additional_reward = 0.0\n",
    "\n",
    "        # if self.num_preprocessings == 1:\n",
    "        # get agent level and position\n",
    "        agent_level, agent_position = get_agent_level_position(obs)\n",
    "\n",
    "\n",
    "\n",
    "        if agent_level is not None and 1 <= agent_level <= len(self.ladder_postion):\n",
    "            agent_level_reward = (7 - agent_level)* (-0.01)\n",
    "            additional_reward += agent_level_reward\n",
    "\n",
    "            if agent_level > self.last_level:\n",
    "                additional_reward += 500.0\n",
    "                self.last_level = agent_level\n",
    "\n",
    "            diff = 0\n",
    "            if agent_position is not None and len(agent_position) > 0:\n",
    "                # get absolute difference between agent position and ladder position\n",
    "                diff = abs(self.ladder_postion[agent_level - 1] - agent_position[0])\n",
    "                agent_position_reward = diff * (-0.1)\n",
    "                additional_reward += agent_position_reward\n",
    "\n",
    "                if action == 2 and diff <= 1 and agent_position[1] > self.last_y_position:\n",
    "                    additional_reward += 50.0\n",
    "                    for i in range(35):\n",
    "                        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "                        if terminated or truncated:\n",
    "                            break\n",
    "                self.last_y_position = agent_position[1]\n",
    "\n",
    "        else:\n",
    "            # If agent_level or agent_position not found, use the previous reward\n",
    "            additional_reward = self.previous_additional_reward\n",
    "\n",
    "        # Round the additional reward to 2 decimal places\n",
    "        additional_reward = round(additional_reward, 2)\n",
    "\n",
    "        # Update the previous reward\n",
    "        self.previous_additional_reward = additional_reward\n",
    "\n",
    "        # Add the additional reward to the original reward\n",
    "        reward += additional_reward\n",
    "\n",
    "        # print(f\"obs: {obs.shape} , Agent Level: {agent_level}, Agent Position: {agent_position}, Additional Reward: {additional_reward}, Total Reward: {reward}\")\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionFilterWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        # The new action space matches the number of allowed actions\n",
    "        self.action_space = Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # Map the reduced action space index to the original action\n",
    "        return self.allowed_actions[act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        # The original shape remains (84,84,1), but the dtype and range change\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=self.observation_space.shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super().__init__(env)\n",
    "        # Check that 'FIRE' is a valid action in the environment\n",
    "        assert 'FIRE' in env.unwrapped.get_action_meanings(), \"Environment does not support 'FIRE' action\"\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3, \"Action space too small for expected actions\"\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Reset the environment\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        # Perform the FIRE action\n",
    "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
    "        if terminated or truncated:  # If game ends after FIRE, reset again\n",
    "            obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs, info\n",
    "        \n",
    "# Custom wrapper to add channel dimension\n",
    "class AddChannelDimension(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape\n",
    "        # Update the observation space to include a channel dimension\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(obs_shape[0], obs_shape[1], 1),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Add a channel dimension\n",
    "        return np.expand_dims(observation, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_env(env_name, allowed_actions, obs_type=\"grayscale\", render_mode=None,):\n",
    "    def _init():\n",
    "        env = gym.make(env_name, obs_type=\"grayscale\", render_mode=render_mode)\n",
    "        print(\"Standard Env.        : {}\".format(env.observation_space.shape))\n",
    "        env = FireResetEnv(env)\n",
    "        print(\"FireResetEnv          : {}\".format(env.observation_space.shape))\n",
    "        # Wrap the environment with the custom ActionFilterWrapper\n",
    "        env = ActionFilterWrapper(env, allowed_actions)\n",
    "        print(\"ActionFilterWrapper   : {}\".format(env.observation_space.shape))\n",
    "        # Wrap the environment to add intermediate rewards\n",
    "        if climb_auto:\n",
    "            env = IntermediateRewardWrapper_climb_auto(env)\n",
    "        else:\n",
    "            env = IntermediateRewardWrapper_climb_not_auto(env)\n",
    "\n",
    "        print(\"IntermediateReward    : {}\".format(env.observation_space.shape))\n",
    "\n",
    "        env = ResizeObservation(env, (84, 84))\n",
    "        print(\"ResizeObservation    : {}\".format(env.observation_space.shape))\n",
    "        env = AddChannelDimension(env)  # Add channel dimension here\n",
    "        print(\"AddChannelDimension  : {}\".format(env.observation_space.shape))\n",
    "        \n",
    "        env = ScaledFloatFrame(env)\n",
    "        print(\"ScaledFloatFrame     : {}\".format(env.observation_space.shape))\n",
    "        \n",
    "\n",
    "        return env\n",
    "    return _init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Env.        : (210, 160)\n",
      "FireResetEnv          : (210, 160)\n",
      "ActionFilterWrapper   : (210, 160)\n",
      "IntermediateReward    : (210, 160)\n",
      "ResizeObservation    : (84, 84)\n",
      "AddChannelDimension  : (84, 84, 1)\n",
      "ScaledFloatFrame     : (84, 84, 1)\n",
      "Post VecFrameStack Shape: (84, 84, 4)\n",
      "Final Observation Space: (4, 84, 84)\n",
      "Render mode after wrapping: rgb_array\n"
     ]
    }
   ],
   "source": [
    "# select relevant actions\n",
    "allowed_actions = [0, 1, 2, 3, 4, 11, 12]\n",
    "# env = DummyVecEnv([make_env(config[\"env_name\"], render_mode=\"rgb_array\")])\n",
    "env = make_vec_env(env_id=make_env(config[\"env_name\"], allowed_actions= allowed_actions,render_mode=\"rgb_array\"), n_envs=1)\n",
    "# stack 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "print(\"Post VecFrameStack Shape: {}\".format(env.observation_space.shape))\n",
    "\n",
    "# convert back to PyTorch format (channel-first)\n",
    "env = MyVecTransposeImage(env)\n",
    "print(\"Final Observation Space: {}\".format(env.observation_space.shape))\n",
    "\n",
    "print(\"Render mode after wrapping:\", env.render_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = PPO.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: [array([657.0702], dtype=float32), array([1444.3865], dtype=float32)]\n",
      "Average Reward: 1050.7283\n"
     ]
    }
   ],
   "source": [
    "rewards_glb = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    frames = []\n",
    "    rewards_episode = []\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # done = terminated or truncated\n",
    "        rewards_episode.append(reward)\n",
    "\n",
    "        frames.append(env.render())\n",
    "\n",
    "    rewards_glb.append(sum(rewards_episode))\n",
    "    # e.g. fps=50 == duration=20 (1000 * 1/50)\n",
    "    imageio.mimwrite(\"model_name\" +'_'+ str(i) +'.gif', frames, duration=20)\n",
    "\n",
    "print(\"Rewards:\", rewards_glb)\n",
    "print(\"Average Reward:\", np.mean(rewards_glb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_prova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
